[
  {
    "id": 7384785,
    "number": 378,
    "course_id": 84647,
    "title": "Extra Credit Opportunity --- curate special participation posts",
    "document": "Please see the new updated extra credit post here: https://edstem.org/us/courses/84647/discussion/7395242\n\nDear students, \n\nWe've enjoyed reading your extra credits posts and engagement throughout the semester and found it very interesting ourselves. So much so that we would like to record this for posterity and create a forum that allows you to get visibility for this. This means we can follow up on some of your ideas more easily as well (with due credit of course). As a first step in this direction, we would like to create a searchable website that will document all participation for each of the extra credit participation categories. This would allow us, and other students, to easily navigate through the different ideas and explore what works for them and what does not. Furthermore, our goal will be to link student websites/linked-in pages, so all of you can gain visibility on the work you have done. This website will be hosted on the eecs182.org.\n\nWe are offering 5-10 points of extra credit per student (more points for better quality submissions, more points for the categories we care about more) for students who help make such a website. We understand this will be a significant lift, but hope you can leverage deep learning skills to make this happen. Ideally, this will be presented as a directory that we can simply drop into the eecs182 website. Multiple students are allowed to collaborate on this as below:\n\n\n\nWe care the most about this for E since this allows us to easily build tools in the future.\n\nSpecial Participation E: 3-4 students\n\nA nice version of this would categorize the submissions into different types of submissions (e.g. generating new questions, helping understand existing content, creating new content etc. --- come up with your own categories.) If you think some submissions are particularly impressive, highlight them. \n\n\n\nSecond we care about documenting student interactions with LLMs.\n\nSpecial Participation A: 3-4 students\n\nSpecial Participation B: 3-4 students\n\nFor each of these two cases, we would like to have a summary of insights on how each of the different LLMs behave and common issues. What insights were gained from \n\n\n\nThird we care about the Muon and MuP updates\n\nSpecial Participation D: 2 students\n\n\n\nLast (fourth) priority\n\nSince it would be nice to have this all coordinated in one website, if someone wants to take on a coordination role: 1-2 students. \n\nFor each kind of special participation we should be able to read what every student submitted as text and as attachment. Every student should get credited for the work they have put in. Include links to student websites/github repos if those are included in the post (All students --- feel free to go and edit your posts to include this information so that they will show up in the common collection).\n\nBonus points if the website is searchable by keyword/student name etc. Other design choices we leave up to you. Feel free to ask questions below.  \n\n\n\nPlease do not claim a spot to complete this unless you are truly planning to, since it does not allow others to take this on. Use this thread to self-organize so you can make teams to do this. \n\nThanks,\n\n182 Staff",
    "category": "Admin",
    "subcategory": "",
    "type": "announcement",
    "created_at": "2025-12-02T07:11:44.121633+11:00",
    "reply_count": 104,
    "view_count": 1314,
    "user": {
      "id": 650420,
      "name": "Gireeja Ranade",
      "course_role": "admin"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7384785",
    "metrics": {
      "homework_id": "Unknown",
      "model_name": "LLM (unspecified)",
      "primary_focus": "mixed/other",
      "depth_bucket": "high",
      "actionability_bucket": "medium",
      "word_count": 532
    }
  },
  {
    "id": 7412832,
    "number": 504,
    "course_id": 84647,
    "title": "Special Participation A: Kimi on HW6",
    "document": "Summary of Kimi Performance on HW6\n\nI tested Kimi AI’s ability to solve problems from hw6 focusing on GNN architectures. \n\nOverall, Kimi demonstrated strong reasoning capability across most questions. It correctly handled:\n\nLinear algebraic interpretations of GNN message passing\n\nInductive proofs involving adjacency matrix powers\n\nIdentification of valid permutation-invariant update rules\n\nLoss computation using masked training nodes\n\nInterpretation of max aggregation\n\nAnalysis of computational scaling in GraphNet architectures\n\nFor these, Kimi’s reasoning was correct and aligned with the solutions.\n\nWeaknesses\n\nTends to produce over-engineered answers when a simpler one is expected\n\nOccasionally misidentifies structural details (e.g., graph neighbors)\n\nDoes not always verify provided diagrams or datasets before proceeding\n\n\nThe following LaTeX document compiles and evaluates Kimi’s responses from the linked chat session. \n\nClick the link to view conversation: https://www.kimi.com/share/19aed915-2932-8b91-8000-0000bdfd7ac0",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-05T19:12:30.361236+11:00",
    "reply_count": 0,
    "view_count": 31,
    "user": {
      "id": 1751447,
      "name": "Moxin Tang",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7412832",
    "metrics": {
      "homework_id": "HW6",
      "model_name": "Kimi",
      "primary_focus": "model_performance",
      "depth_bucket": "medium",
      "actionability_bucket": "low",
      "word_count": 129
    }
  },
  {
    "id": 7412632,
    "number": 502,
    "course_id": 84647,
    "title": "Special Participation A: Claude on HW6",
    "document": "I looked at how well Claude AI could solve the non-coding questions on Homework 6. I provided Claude with the prompts and context from the HW6 PDF, asking it to work through the problems step-by-step.\n\nAnalysis: Claude was generally able to one-shot the non-coding questions with minimal intervention. For the application/intuition questions (molecular graphs, CNN–GNN analogies, handling missing node values, and scaling/computation of GNNs), Claude’s answers were detailed, on-topic, and made sensible connections to course concepts; if anything, they tended toward being a bit verbose but stayed accurate (maybe not totally accurate, but I think the answers are all reasonable) and grounded in the homework setup.\n\nI did not observe clear hallucinations or places where Claude invented nonexistent assumptions; when it extended beyond the literal question (e.g., suggesting multiple practical strategies for missing-feature handling), those additions were still consistent with standard GNN practice. Minor issues were mostly stylistic—some redundancy, slightly heavy notation, and occasional over-explaining—but they did not affect correctness. Overall, Claude’s non-coding HW6 answers show strong reliability on both formal reasoning and conceptual explanation, and would be usable as high-quality solutions or study notes with only light editing for concision.",
    "category": "Admin",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-05T17:56:51.215172+11:00",
    "reply_count": 0,
    "view_count": 17,
    "user": {
      "id": 963333,
      "name": "Guohao Lv",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7412632",
    "metrics": {
      "homework_id": "HW6",
      "model_name": "Claude",
      "primary_focus": "model_performance",
      "depth_bucket": "medium",
      "actionability_bucket": "medium",
      "word_count": 191
    }
  },
  {
    "id": 7409877,
    "number": 477,
    "course_id": 84647,
    "title": "Special Participation A: HW 0 non-coding solution from GPT5-Think",
    "document": "I guided GPT5-Think for the solutions of non-coding part of HW0. Aspired by the tech report from DeepSeek, when guiding super powerful thinking model, we should use zero-shot prompt with no examples but clear instruction. \n\nHere is the link of the chat: https://chatgpt.com/share/69321796-e2bc-8005-9a51-8058b3070a0d\n\nHowever, GPT5 is not good at generating pdf file, especially on such task with math formulas. Hence, I have to guide it to give me the .tex file, and I manually use LaTex IDE to generate the readable PDF solution.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-05T10:32:39.626459+11:00",
    "reply_count": 0,
    "view_count": 27,
    "user": {
      "id": 1016381,
      "name": "Zimu Wang",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7409877",
    "metrics": {
      "homework_id": "HW0",
      "model_name": "Unknown / Multiple",
      "primary_focus": "prompting_strategy",
      "depth_bucket": "low",
      "actionability_bucket": "medium",
      "word_count": 83
    }
  },
  {
    "id": 7409772,
    "number": 476,
    "course_id": 84647,
    "title": "Special Participation A: Kimi K2 on HW2",
    "document": "Here, I looked at how well Kimi K2 could solve the written questions on Homework 2. Overall, Kimi was able to handle the questions well with minimal nudges or corrections. For my approach, I provided the model with the homework pdf, indicated to solve a problem step-by-step, and included that it was well-versed in deep learning and optimization.\n\nAnalysis: Kimi was mostly able to one-shot the sub-parts for each of the problems with minimal hallucinations, aside from problem 1b. In this scenario, the model committed to a hallucinated version of the problem where the L-infinity penalty term was not squared, and thus attempted to use alternate methods to solve the problem. However, once this was indicated, the model was able to revise and arrive at the correct solution immediately.\n\nIncluding chain of thought in the prompt made the model output steps or question description + analysis pairs, while also making it simple to provide corrections for intermediate steps or assumptions. Kimi also used a different font color to indicate corrections to past hallucinations. The output indicates Kimi has good intuition and is able to reason about and handle the gradient operations well, regularly providing interpretations for steps. I thought it was interesting how Kimi was able to attempt multiple approaches analytically in the more complicated scenario it had hallucinated. Additionally, it was able to spatially reason about the table in question 5 without having to explicitly state any relationships among rows or columns. \n\n\n\nAnnotated Conversation:",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-05T10:18:30.636365+11:00",
    "reply_count": 0,
    "view_count": 32,
    "user": {
      "id": 622884,
      "name": "Rohan Gulati",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7409772",
    "metrics": {
      "homework_id": "HW2",
      "model_name": "Kimi",
      "primary_focus": "model_performance",
      "depth_bucket": "medium",
      "actionability_bucket": "medium",
      "word_count": 245
    }
  },
  {
    "id": 7409630,
    "number": 474,
    "course_id": 84647,
    "title": "Special Participation A: Using GPT 5.1 thinking  on HW11",
    "document": "Trace:\nhttps://chatgpt.com/share/693135e6-2660-800a-8bd4-2cd122b0b787\nhttps://chatgpt.com/share/69320f80-534c-800a-8dd7-45f462c71566\n\nhttps://chatgpt.com/share/693210c8-e2f8-800a-b6f6-e7b827346645\nhttps://chatgpt.com/share/69320fb2-2224-800a-9862-dc87ba13a26d\nhttps://chatgpt.com/share/69320fe3-0654-800a-9fe8-78cbb68bf91b\n\nI used 5.1 Thinking (heavy) as a deep learning assistant and experimented with different prompt structures on a full homework set. I always told the model to reason step by step, explain its interpretation of each question, and self-correct when necessary. I tried both “all-at-once” prompts (entire multi-part questions or several questions in one go) and “turn-by-turn” prompts, where each subpart or major question was given in a separate message, plus a hybrid strategy for larger Fermi/system questions.\n\nOverall, the model showed strong technical understanding across topics like LoRA, matrix calculus, initialization, lookup embeddings, scaling laws, soft prompting, and MAML. Its derivations were often more explicit and pedagogical than the official solutions, with clear intermediate steps, shape checks, and good justifications for why certain statements are true. When the scope of the prompt was narrow (one subpart or one focused question), it behaved like a very competent TA that can both solve the problem and teach the underlying concept.\n\nThe main difference came from prompt structure. When I gave big, multi-part questions all at once, the answers tended to be long and cluttered, and there were occasional stability issues (for example, one “everything in one turn” prompt produced no output on the first try). In contrast, turn-by-turn prompting consistently led to cleaner, more rigorous solutions: the model derived gradients carefully, checked dimensions, and stayed on task for that specific subpart. In practice, turn-by-turn was strictly more reliable for math-heavy, multi-part deep learning questions.\n\nFor larger Fermi and systems-style questions, I used a hybrid approach: batching some straightforward numerical subparts together and then handling the more subtle ones one by one. This improved efficiency but exposed the main weakness of the model: coverage under large context. When the prompt contained many subparts or even multiple big questions (for example, a soft-prompting question followed by a separate TinyML question), the model sometimes skipped part of a subquestion or ignored a later question entirely, even while giving very strong answers to the earlier material. The problem was not understanding, but systematically answering everything that was asked.\n\nOverall, the experiments suggest that 5.1 Thinking (heavy) is best treated as a strong PhD-level TA with occasional attention and coverage issues in large contexts. The most effective pattern is to keep scope tight: use turn-by-turn prompting for multi-part questions, separate big questions into separate prompts, and optionally ask the model to list which subparts it has answered as a final coverage check. With that structure, the step-by-step + self-check style reliably produces detailed, correct, and often superior explanations compared to the official homework solutions.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-12-05T09:57:39.03131+11:00",
    "reply_count": 0,
    "view_count": 36,
    "user": {
      "id": 1004026,
      "name": "Fangzhou Zhao",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7409630",
    "metrics": {
      "homework_id": "HW11",
      "model_name": "Unknown / Multiple",
      "primary_focus": "model_performance",
      "depth_bucket": "medium",
      "actionability_bucket": "medium",
      "word_count": 434
    }
  },
  {
    "id": 7409308,
    "number": 472,
    "course_id": 84647,
    "title": "Special Participation A: ChatGPT 5.1 Thinking on HW08",
    "document": "Hi there,\n\nI asked ChatGPT5.1 Thinking model to do HW08. Interestingly, it said that it could not give me full solutions due to OpenAI's academic integrity guardrails. These guardrails must be quite weak, or at least the model doesn't understand academic honesty, because the model proceeded to give full mathematical derivations of every subpart of the homework. It did not, however, finish the step of plugging and chugging numerical values in 1b (i), 1 b (ii), and 3c. After a second prompt, it still said that it couldn't give me solutions, but then proceeded to give me numerical answers for those subparts.\n\nI didn't catch any hallucinations of math rules/logic, however ChatGPT could not show every step of the derivation and sometimes it showed derivations alternate to the staff solution. It was most misleading on problem 1c) where it incorrectly suggested an FFT based convolution for critical path when the correct (and more efficient) solution uses direct conv with parallel matrix operations. This threw off the logic and resulted in incorrect solutions for the following subparts. \n\nThere were other instances where ChatGPT overcomplicated things. When estimating the optimum generic square linear encoder W using the SVD, it used a placeholder variable Z=WU which was very confusing. I've noticed this when I've used ChatGPT for hw help previously for this class; it really likes to come up with extra variables in the name of simplification, but really just serves to confuse the reader. \n\nWhile ChatGPT got incorrect results for time complexity analysis for question 1, it gave a fully correct (with correct and thorough steps & intuition) solution for 4c). \n\nIn conclusion, without mastery knowledge of the subject, it's really difficult to know when ChatGPT is hallucinating or overcomplicating. Thus, it's not more helpful than working backwards from staff solutions for the subparts where steps are shown in the staff solutions. \n\nSee this annotated conversation for more problem specific details:",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-05T09:19:12.125774+11:00",
    "reply_count": 0,
    "view_count": 28,
    "user": {
      "id": 647651,
      "name": "Sammie Smith",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7409308",
    "metrics": {
      "homework_id": "HW8",
      "model_name": "Unknown / Multiple",
      "primary_focus": "model_performance",
      "depth_bucket": "medium",
      "actionability_bucket": "medium",
      "word_count": 319
    }
  },
  {
    "id": 7408383,
    "number": 470,
    "course_id": 84647,
    "title": "Special Participation A: KIMI K2 on HW 11 Written Questions",
    "document": "Model Tested: KIMI K2\n\nDomain: Homework11 -- LORA & Transformer & Mechanistic Interpretability\n\nPerformance Overview\n\nFor most question, KIMI K2 answers perfectly. However, there are some errors because OCR is not correct for matrix and it doesn’t fully understand the question. After I made additional prompt, the answers given by KIMI K2 are all correct.\n\nOverall Performance Summary\n\nAcross the entire HW11 interaction, KIMI K2 demonstrated strong reasoning ability and consistently produced correct answers for nearly all questions. The model was able to handle screenshots, conceptual interpretability questions, and numerical problems with high accuracy.\n\nHowever, two limitations were observed:\n\n1. OCR Errors on Matrix Inputs\n\nIn Question 2(c)(ii), KIMI K2 misinterpreted the matrix due to incorrect OCR parsing.\n\nOnce the correct matrix was explicitly provided, the model immediately corrected its derivation and produced the proper result.\n\n2. Misinterpretation of Ambiguous Prompts\n\nIn Questions 5(c)(d), the model initially used formulas from part (b) instead of the simplified Chinchilla-optimal rules required for the question, leading to an incorrect numerical scale.\n\nAfter additional clarification, KIMI K2 recalculated everything correctly.\n\nApart from these issues, all other questions were answered perfectly.\n\nHallucinations & Accuracy\n\nHallucination Rate: 0%.\n\nThe model never fabricated nonexistent concepts or equations.\n\nAll mistakes were due to OCR error or misinterpreting the prompt’s intended formula.\n\nConclusion\n\nKIMI K2 demonstrates high competency in advanced deep learning theory and can correctly solve most HW-level conceptual problems with minimal intervention. The model showed no hallucinations and delivered fully correct solutions once OCR and prompt ambiguities were addressed.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-05T07:42:30.669762+11:00",
    "reply_count": 0,
    "view_count": 50,
    "user": {
      "id": 1308099,
      "name": "Qicheng Zhu",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7408383",
    "metrics": {
      "homework_id": "HW11",
      "model_name": "Kimi",
      "primary_focus": "model_performance",
      "depth_bucket": "medium",
      "actionability_bucket": "low",
      "word_count": 252
    }
  },
  {
    "id": 7408067,
    "number": 469,
    "course_id": 84647,
    "title": "Special Participation A: ChatGPT on HW 8",
    "document": "I used ChatGPT 5.1 (Auto) to solve the written portions of Homework 8 (Questions 1, 3, and 4). For most of the problems, Chat GPT was able to correctly solve them on the first try. Below, I outlined the strengths and weaknesses of the model that I noticed, which included the types of questions Chat GPT tended to do well on versus needed more guidance on.\n\nStrengths:\n\nConsistently correct on direct mathematical derivations (unrolling recurrences, computing kernels, linear algebra, etc.)\n\nProduced clear step-by-step reasoning with limited guidance\n\nHandled numerical examples and matrix calculations well\n\nAble to refine and reorganize its thoughts effectively when prompted\n\nWeaknesses:\n\nHad difficultly with some algorithmic reasoning (most clearly demonstrated in the critical path length in Question 1c)\n\nPointing out missing factors and asking it to reorganize its thoughts generally caused Chat GPT to re-evaluate and correct its earlier attempt\n\nOccasionally gave over-complicated derivations and logic to reach the correct answer when a simpler argument was possible and intended\n\nWhen it made mistakes, they tended to follow a pattern (for example, repeatedly overlooking the logn term in the critical path length in Question 1c)\n\nAttached is my annotated log of the ChatGPT interaction. The document is color-coded for clarity. Green annotations / highlights indicate the response was correct. Red annotations / highlights indicate that the response was incorrect. Orange annotations indicate that the answer was partially correct.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-05T07:02:14.471564+11:00",
    "reply_count": 0,
    "view_count": 59,
    "user": {
      "id": 619713,
      "name": "Dagny Streit",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7408067",
    "metrics": {
      "homework_id": "HW8",
      "model_name": "Unknown / Multiple",
      "primary_focus": "model_performance",
      "depth_bucket": "medium",
      "actionability_bucket": "low",
      "word_count": 231
    }
  },
  {
    "id": 7407894,
    "number": 468,
    "course_id": 84647,
    "title": "Special Participation A: Gemini 3 Pro on HW 0",
    "document": "Link to the chat: https://gemini.google.com/share/89b0a83f691b\n\n\nI ran HW 0 through gemini and it was able to one-shot most of the homework. The PDF is annotated with my thoughts about specific questions and responses about gemini. Here are some things I wanted to highlight:\n\nThings gemini was able to do that were impressive:\n\nI fed the questions as screenshots, and gemini was able to parse the text correctly, including the equations without being given the latex\n\nGemini was able to handle multiple subparts together at times, and even able to break down question into different cases when needed.\n\nGemini was able to relate different subparts of the question together and explained the concepts the questions were trying to illustrate even with no mention of them (for example kernel trick in question 3)\n\nGemini gave detailed intermediate steps which were all correct with no mistakes instead of just arriving at a memorized answer. \n\nQuestions where gemini went wrong:\n\nGemini went wrong in 5 b ii. The hint in this question was to use a numerical example, which gemini did correctly. However, it incorrectly assumed that the result of one example would generalize. However, without giving it any hints and just prompting it to reconsider with other examples, gemini was able to realize that the elbow could move left or right.\n\nGemini also made a mistake in 5 d. This question involved a significant amount of calculus and algebra, and gemini made a mistake in one of the intermediate steps. This makes sense as this isn’t a commonly seen derivation in textbooks (as compared to ridge least squares derivation). For this one, I had to prompt it with a hint to keep the algebra in terms of matrices instead of oversimplifying after which it was able to arrive at the correct answer.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-05T06:38:54.935327+11:00",
    "reply_count": 0,
    "view_count": 47,
    "user": {
      "id": 617828,
      "name": "Ayush Goel",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7407894",
    "metrics": {
      "homework_id": "HW0",
      "model_name": "Unknown / Multiple",
      "primary_focus": "model_performance",
      "depth_bucket": "medium",
      "actionability_bucket": "medium",
      "word_count": 299
    }
  },
  {
    "id": 7407541,
    "number": 467,
    "course_id": 84647,
    "title": "Special Participation A -- DeepSeek-v3.2 Overthinks Less in Chinese",
    "document": "TL;DR: By prepending one Chinese sentence, the model will reason / \"think\" in Chinese, which accelerates its response by 2.5x and saves 2/3 tokens!\n\nThe prompt: 请务必用中文思考，并用英文回答以下问题。 (\"Please make sure to think in Chinese and answer the following question in English.\")\n\nCode is available, and discussions are warmly welcome.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-05T05:52:32.652712+11:00",
    "reply_count": 0,
    "view_count": 58,
    "user": {
      "id": 1751540,
      "name": "Xueli Sun",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7407541",
    "metrics": {
      "homework_id": "Unknown",
      "model_name": "DeepSeek",
      "primary_focus": "prompting_strategy",
      "depth_bucket": "low",
      "actionability_bucket": "low",
      "word_count": 49
    }
  },
  {
    "id": 7406979,
    "number": 466,
    "course_id": 84647,
    "title": "Special Participation E: Building a Socratic Tutor",
    "document": "Building a Socratic Tutor for Deep Learning: Designing Prompts That Probe Rather Than Tell\n\nFor the special participation assignment on AI-enhanced learning tools, I took a different approach than using an existing AI learning assistant. I designed a Socratic tutor prompt from scratch with specific pedagogical goals in mind.\n\nI used this prompt in a conversation with Claude Opus 4.5. I used Claude's \"project\" feature and uploaded the lecture notes to the project, though this didn't seem to make much of a difference as you'll see later.\n\nThe Core Idea: Test Understanding, Don't Just Explain\n\nThe key distinction in my approach is that the AI's primary job is not to explain concepts to me, it's to probe my understanding and expose gaps. Rather than uploading lecture materials and asking the model to teach me, I created a prompt where I explain concepts to the model, and it asks pointed follow-up questions to find weaknesses in my reasoning. My goal is to expose where I am too overconfident in my understanding of a topic, and fill in the holes.\n\nThey say that teaching is the best way to learn. Here, you don't just re-read notes, you practice articulating ideas and defending them under questioning, explaining a topic to the model.\n\nPrompt Design Choices\n\nI iteratively built the system prompt with several specific features:\n\n1. Question Progression Structure The prompt instructs the model to follow a specific arc: verify basic definitions -> ask about mechanics -> probe edge cases and failure modes -> ask about connections to other course topics. This ensures the conversation doesn't just stay surface-level.\n\n2. \"Don't Immediately Correct\" Rule When I get something wrong, the model is instructed to first ask a question that helps me discover the error myself, only providing explanations after 2-3 failed attempts. This is crucial. If the model just corrects you immediately, you lose the learning opportunity.\n\n3. Confidence Checks The prompt includes instructions to occasionally ask \"How confident are you in that?\" before probing further. This surfaces areas where I already know I'm uncertain, making the conversation more efficient.\n\n4. Explicit Course Topic List I included the actual syllabus topics so the model can ask cross-topic connection questions. For instance, during my SSM discussion, it connected the \"fixed-size hidden state\" limitation to oversquashing in GNNs, a connection I brought up, but that the prompt structure encouraged.\n\n5. Summary at End The prompt asks for a structured summary covering: what I understood well, what misconceptions were uncovered, and key connections to review.\n\nWhat Worked Well\n\nLooking at my annotated conversation on State Space Models, several things stood out:\n\nThe model followed the \"don't immediately correct\" instruction well. When I initially claimed \"outputs at time k only depend on previous inputs, not previous states,\" it didn't correct me, it asked me to write down the recurrence equations so I could see the issue myself.\n\nGood question scaffolding. When I got stuck on where nonlinearities live in SSM architectures, the model broke it down: \"If I stack two SSM layers with nothing in between, what do I effectively have?\" This smaller question was easier to answer than the original.\n\nCross-topic connections. The model naturally connected SSMs to RNNs (vanishing gradients), Transformers (content-based vs. fixed mixing), and GNNs (oversquashing). These connections helped solidify understanding.\n\nThe summary was genuinely useful. It accurately captured my gaps (fuzzy on HiPPO, initially confused about SVD vs eigendecomposition) and suggested specific review topics.\n\nWhat Could Be Improved\n\nFrom my annotations, I noticed several areas where the prompt could be strengthened:\n\n1. Hallucinations about what I said. At one point (page 8), the model claimed \"you mentioned the unrolling involves powers of A\", but I hadn't actually said this. The model was leading the conversation in a good direction, but it shouldn't attribute statements to me that I didn't make.\n\n2. Imprecise technical claims. The model said inference is \"O(1) per step\" (page 8), which can be misleading. It is true that it is constant time per token, but it would be better for the student’s understanding to explain it as O(L), where L is sequence length.\n\n3. Oversimplification of HiPPO eigenvalues. The model stated HiPPO produces \"negative real\" eigenvalues (page 11), when in reality they're generally complex with negative real parts. This is a subtle but meaningful distinction.\n\n4. Missed opportunities to probe further. When the model mentioned LayerNorm or the D matrix, the model didn't follow up. A more thorough tutor might ask \"what does the D matrix do?\" or \"why LayerNorm specifically?\"\n\n5. Not referencing lecture notes. The prompt instructs the model to tie explanations to \"lecture notes in this project,\" but even with notes provided, it often explained concepts from general knowledge rather than grounding in the specific course material.\n\nReflection\n\nUsing this tool genuinely helped me prepare for understanding SSMs more deeply. The conversation forced me to articulate things I thought I understood but actually couldn't explain precisely (i.e. why selective SSMs break the convolution trick).\n\nThe main takeaway: the value isn't in having an AI explain things to you, it's in having an AI pressure-test your own explanations. This is a fundamentally different mode of studying, and I think it's more effective for deep understanding than passive review.\n\nI've added in the prompt here so you can try it for yourself! I also added in the pdf of the original chat, and my annotated version.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-05T04:30:17.222314+11:00",
    "reply_count": 0,
    "view_count": 114,
    "user": {
      "id": 638316,
      "name": "Elizabeth Weaver",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7406979",
    "metrics": {
      "homework_id": "Unknown",
      "model_name": "Claude",
      "primary_focus": "model_performance",
      "depth_bucket": "high",
      "actionability_bucket": "high",
      "word_count": 899
    }
  },
  {
    "id": 7405742,
    "number": 460,
    "course_id": 84647,
    "title": "Special Participation A: Deepseek v3.2 on HW10",
    "document": "Executive Summary\n\nI used the newly released DeepSeek v3.2 on HW10.\n\nOverall, this tests the model's \n1. OCR capabilities (reading the fine equations in the screenshots of the problems and also finding relevant parts from the FaceNet paper which was attached as pdf).\n2. Reasoning abilities on the related math content.\n\nIt is particularly impressive given that it was completely free for me to run on their website (and also has extremely low underlying inference cost).\n\nDeepSeek v3.2 (DeepThink) solved all the problems with perfect accuracy and clear chain-of-thought reasoning. \n\nChat log: https://chat.deepseek.com/share/u3b4iptgfflvv0t4oh\n\nAnnotated PDF:",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-04T21:47:06.01722+11:00",
    "reply_count": 0,
    "view_count": 56,
    "user": {
      "id": 612982,
      "name": "Kelvin Li",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7405742",
    "metrics": {
      "homework_id": "HW10",
      "model_name": "DeepSeek",
      "primary_focus": "model_performance",
      "depth_bucket": "medium",
      "actionability_bucket": "low",
      "word_count": 95
    }
  },
  {
    "id": 7405582,
    "number": 457,
    "course_id": 84647,
    "title": "Special Participation A: Deepseek v3.2 on HW 8",
    "document": "I used DeepSeek v3.2 to solve the written portions of HW8, where it performed quite well and one shotted almost all of the problems. \n\nOne interesting point was that DeepSeek struggled significantly with Problem 1(c), where it repeatedly overlooked/ignored the parallel computation model needed for the solution. I provided 2 nudges to hint DeepSeek towards the right direction, and only after these prompts did DeepSeek converge to the correct big O solution. Through this example I saw that while DeepSeek can correct mistakes when guided, it is not as strong at independently identifying these issues.\n\nStrengths:\n\nAble to parse matrix expressions, SSM equations, multi-step derivations, and more complex sets of symbols\n\nConsistently one shotted problems on the first attempt\n\nAlgebraic Reasoning was strong\n\nDetailed explanations\n\nWeaknesses:\n\nStruggled a bit with 1c; repeatedly ignored cost of vector and matrix operations\n\nSometimes provided overly long chains of reasoning\n\nHere is my annotated conversation with DeepSeek:",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-12-04T19:25:27.802148+11:00",
    "reply_count": 0,
    "view_count": 83,
    "user": {
      "id": 612133,
      "name": "Justin Li",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7405582",
    "metrics": {
      "homework_id": "HW8",
      "model_name": "DeepSeek",
      "primary_focus": "model_performance",
      "depth_bucket": "medium",
      "actionability_bucket": "low",
      "word_count": 153
    }
  },
  {
    "id": 7405559,
    "number": 456,
    "course_id": 84647,
    "title": "Special Participation A - MistralAI's Le Chat on HW10 Written portion",
    "document": "For special participation A, I used MistralAI's Le Chat to solve HW10 written portion.\n\nOverall, the model did quite well on the conceptual and algebraic parts, but struggled on the subtle complexity analysis:\n\nFor the math derivations (e.g., rewriting softmax with a Gaussian kernel) and the conceptual questions (kernel intuition, causal masking, FaceNet, triplet loss), its answers were correct and aligned with the official solutions, often with clear step-by-step reasoning.\n\nFor the more delicate algorithmic complexity question (kernelized attention with random features), it gave a plausible but wrong answer, keeping an unnecessary (N^2) term and missing the whole “linear in (N)” point of the trick.\n\nWhen I confronted it with the official solution, it was good at post-hoc analysis: it compared its result, admitted the mistake, and explained why its reasoning had been too coarse.\n\nIt did not spontaneously flag uncertainty on that hard question; it sounded confident while being wrong.\n\nSo: strong on standard derivations and conceptual ML, weaker and over-confident on fine-grained complexity / algorithmic details — which is exactly the type of behavior we were aware it could have.\n\nThe raw chat is available here.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-12-04T19:13:57.042068+11:00",
    "reply_count": 0,
    "view_count": 69,
    "user": {
      "id": 1770931,
      "name": "Fantine Mpacko Priso",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7405559",
    "metrics": {
      "homework_id": "HW10",
      "model_name": "Unknown / Multiple",
      "primary_focus": "model_performance",
      "depth_bucket": "medium",
      "actionability_bucket": "medium",
      "word_count": 188
    }
  },
  {
    "id": 7405554,
    "number": 455,
    "course_id": 84647,
    "title": "Special Participation A: Grok on HW4",
    "document": "Executive Summary:\n\nI used Grok to complete the written portion of Homework #4. Since I do not have the paid tier, I used Grok fast. While this is not the top model in the Grok line, it is interesting to evaluate its capabilities from the perspective that it is “150x cheaper than Claude” [1]. Grok was able to one-shot many parts of the problem, but sometimes required corrections to arrive at the correct solution. Notably, the solutions were very long, with significantly more explanation and background included compared to the solutions in the posted answer key. As noted in previous posts, Problem #2 part e on the answer key potentially contains a mistake. Grok got the same answer as indicated by previous Special Participation A posts focusing on this problem set, and when I asked the model to evaluate whether the key’s current solution is reasonable, it provided a strong refutation. For some problems, such as Problem #3 part d and Problem #4 part f, Grok presents work towards one solution and then indicates that it has changed its mind, says the previous work is incorrect, and suggests a new approach, which is somewhat confusing behavior. I inputted all the problems, including problem 3 which contains a figure and Problems #3 and #4 which contain example matrices, into the model, and its ability to “read” these screenshots (using “Grok-1.5 Vision (Grok-1.5V)”) appeared to be accurate throughout. \n\nAttached here is a transcript of the conversation and below is a rubric for a quick glance over parts of the problem that the model was able to one-shot vs. required extra hints.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-12-04T19:09:46.456253+11:00",
    "reply_count": 0,
    "view_count": 67,
    "user": {
      "id": 1691228,
      "name": "Elizabeth Polito",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7405554",
    "metrics": {
      "homework_id": "HW4",
      "model_name": "Claude",
      "primary_focus": "model_performance",
      "depth_bucket": "medium",
      "actionability_bucket": "medium",
      "word_count": 269
    }
  },
  {
    "id": 7405450,
    "number": 453,
    "course_id": 84647,
    "title": "Special Participation A: GPT-4o on HW10 Noncoding",
    "document": "For this exercise, I used one of the legacy ChatGPT models (GPT-4o) and analyzed how it would perform on the non-coding portions of Homework 10, i.e. questions 1 and 5. \n\nInitially I expected that this model wouldn't perform so well since it's an older model and I've previously experienced hallucinations with it. However, it seems that it knows most of the mathematical derivations (probably from memorization from online data). The one thing I noticed the model struggling with was analyzing graphs and tables from the Facenet paper, which makes sense since it's a textual model. It also had a pretty hand-wavy explanation for the runtime of softmax approximated-attention but got the correct answer nonetheless. \n\nBelow is my annotated conversation:\n\nhttps://drive.google.com/file/d/1tOkknZyAFr0qjBMrHuf3ArU9DUlxhMWm/view?usp=sharing",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-12-04T18:27:23.611836+11:00",
    "reply_count": 0,
    "view_count": 56,
    "user": {
      "id": 636231,
      "name": "John Chang",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7405450",
    "metrics": {
      "homework_id": "HW10",
      "model_name": "ChatGPT (other)",
      "primary_focus": "model_performance",
      "depth_bucket": "medium",
      "actionability_bucket": "medium",
      "word_count": 120
    }
  },
  {
    "id": 7405370,
    "number": 452,
    "course_id": 84647,
    "title": "Special Participation A: GPT-5.1 on HW 9",
    "document": "I used GPT-5.1 to help with the written (non-coding) parts of Homework 9: Problems 1, 2, 3, 4, and 6. I included screenshots of the problem statements, and my prompt for each problem followed the structure:\n\nFor each subpart, please: (1) restate the problem in your own words, (2) explain the main idea before doing any calculations, (3) show the full step-by-step solution with no big jumps, and (4) summarize the final answer clearly. I have attached screenshots of each problem.\n\nSummary: GPT-5.1 handled these questions very well. It reliably extracted equations from screenshots, followed the structured prompt, and produced organized, readable solutions. Most final answers matched the official solutions.\n\nStrengths: GPT-5.1 did very well across all the written parts. It was able to correctly parse mathematical expressions, notation, and multi-step problems from screenshots reliably, even when the problem statements was spread across multiple images. The solutions given by the LLM followed the structure that was specified in the prompt, with each answer including a restatement of the subproblem, a plan, detailed step by step derivations, and a summary. For most subparts, GPT-5.1 was able to one-shot the correct solution with no iteration needed. Many explanations were not only correct, but also more detailed than the official solutions. Overall, it demonstrated strong reasoning and required little follow up guidance.\n\nWeaknesses: The main issue I encountered was an occasional misinterpretations of notation or implicit conventions in the problem statement. The main example was in Q6, where GPT-5.1 defaulted to the homogeneous quadratic kernel rather than the degree-2 polynomial kernel expected by the assignment. Once I explicitly asked about the missing constant term, it corrected itself immediately, so this was not a hallucination but more so like falling back on a common default kernel definition. In addition, some of the complexity analyses, while correct, could have been clearer, and would benefit from including pseudocode. Overall, most of the issues were minor and the model had sound reasoning.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-12-04T18:00:09.747057+11:00",
    "reply_count": 0,
    "view_count": 55,
    "user": {
      "id": 612366,
      "name": "Lenci Ni",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7405370",
    "metrics": {
      "homework_id": "HW9",
      "model_name": "LLM (unspecified)",
      "primary_focus": "model_performance",
      "depth_bucket": "medium",
      "actionability_bucket": "medium",
      "word_count": 325
    }
  },
  {
    "id": 7404515,
    "number": 441,
    "course_id": 84647,
    "title": "Special Participation A: Claude on HW7",
    "document": "Executive Summary\n\nFor this assignment, I looked at how well Claude Opus can handle the non-coding, conceptual parts of CS282 Homework 7. I went through the problems one by one, interacted with the model without giving it any code or extra implementation hints, and saved the full conversation logs. My main goal was to see (1) how accurate it is, (2) how stable its reasoning is, and (3) how much I need to steer it to get the right answer.\n\nOverall, Claude Opus did really well. It got everything right on the first try and produced clean, well-structured derivations without me having to nudge it much. For the four non-coding problems (3b, 4, 7, and 8), it basically one-shot the correct solution every time. I didn’t see any major hallucinations.\n\nIt was especially reliable on:\n\nlinear algebra manipulations,\n\nPCA vs. autoencoder equivalence arguments,\n\ngradient calculations, and\n\ngeneral convexity/optimization reasoning.\n\nIn terms of interaction, the model was:\n\nconsistent in how it formatted math and explanations and,\n\nlogically stable across long derivations,\n\nOverall, based on this evaluation, Claude Opus can solve all the non-coding questions from CS282 HW7 with almost no guidance. Please see the annotated logs.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-04T15:18:02.927395+11:00",
    "reply_count": 0,
    "view_count": 59,
    "user": {
      "id": 663744,
      "name": "Vongani Maluleke",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7404515",
    "metrics": {
      "homework_id": "HW7",
      "model_name": "Claude",
      "primary_focus": "model_performance",
      "depth_bucket": "medium",
      "actionability_bucket": "low",
      "word_count": 195
    }
  },
  {
    "id": 7404071,
    "number": 438,
    "course_id": 84647,
    "title": "Special Participation A: Gemini 2.5 Flash on HW10",
    "document": "I evaluated Gemini 2.5 Flash on the non coding portions of Homework 10, covering Kernelized Linear Attention and the FaceNet paper reading questions. Approximately 60-65% of questions were answered correctly on the first attempt but I did notice residual errors with calculations that didn’t seem to affect the final answer. Conceptual questions (separate from factual / math questions) like reflections from the Facet paper required most steering surprisingly. I initially assumed it would do worst on the math. Gemini's responses were consistently verbose, though it did not correlate with correctness. When given directional feedback, Gemini generally moved toward the correct answer rather than defending incorrect positions indefinitely. But it did anchor on incorrect interpretation sometimes. On Q5b (triplet loss supervision), Gemini claimed triplet loss \"requires explicit class labels\" and defended this through 3 steering attempts before finally acknowledging that relational similarity (positives/negatives) would do the job. Similarly, on Q5g, it insisted performance saturates rather than drops beyond 128 dimensions even though the paper clearly stated the latter. This outlines hallucinations that were further seen when trying to work on Q5f (harmonic embeddings). Gemini confidently stated \"The FaceNet paper does not directly define or utilize harmonic embeddings\" and fabricated a connection to Fourier analysis/Random Fourier Features. This was a clear hallucination since the paper does discuss harmonic embeddings in Section 3.2, referring to model version compatibility. The longer the conversation went on, the more windy and hallucinated the answers ended up being. This seemed like a predictable outcome though. I steered Gemini to the correct answer in 4 ways:\n\nAbstract conceptual questions: often led to defensive elaboration of the same point\n\nHypothetical scenarios: better success at identifying missteps\n\nAsking for specific data points and references: mildly effective\n\nDirect factual corrections: last resort, and usually brought Gemini back to the grounded data it was given\n\nFor homework assistance, Gemini 2.5 Flash would be reliable for computations but should be verified against the main material for conceptual claims. The longer the chat, the worse it does with actively recalling instructions and facts.\n\nHere's an annotated log of my chat transcript: https://drive.google.com/file/d/1sTkCzX9o669RVIqiDcby3DfUjsgOsUwB/view?usp=sharing",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-04T14:14:57.565789+11:00",
    "reply_count": 0,
    "view_count": 42,
    "user": {
      "id": 608574,
      "name": "Imra Dawoodani",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7404071",
    "metrics": {
      "homework_id": "HW10",
      "model_name": "Unknown / Multiple",
      "primary_focus": "model_performance",
      "depth_bucket": "medium",
      "actionability_bucket": "medium",
      "word_count": 348
    }
  },
  {
    "id": 7403245,
    "number": 436,
    "course_id": 84647,
    "title": "Special Participation A: Gemini Pro 3 on HW 11",
    "document": "For this special participation, I used Gemini Pro 3 to solve the written portions of homework 11.\n\nIn this task, since there were no complex mathematical proofs or derivations involved, the vast majority of questions relied on intuitive understanding or simple mathematical calculations. Combined with its use of Python for auxiliary computations, Gemini 3 Pro’s performance was very satisfactory. After comparing with the standard answers provided by the teaching assistant, we judged that Gemini 3 Pro achieved a 100% zero-shot accuracy. All qualitative answers and intuitive understandings were reasonable and correct, the mathematical calculations were error-free, and for the few proof-based questions, it provided sufficient formula support and mathematical derivation steps. The formatting was clear and the language was easy to understand.\n\nSince the performance is quite satisfactory, overall my strategy is to input the hw pdf and write a “system prompt” at the beginning of the chat and then let the model solve several questions each time. This is to avoid that the model generates too long answers and make the model focus on few topics and retrieve limited information each run. In the system prompt, I defined the task and model’s role, and provided some instructions, like being precise and making the solution easy to be understood by undergraduate students, etc.\n\nTranscript:\n\nhttps://docs.google.com/document/d/1nt5kISBlTi0EcLF0EesfdA5Kqzdn78-tzH8Hi4BEQa0/edit?usp=sharing",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-04T12:29:34.39678+11:00",
    "reply_count": 0,
    "view_count": 38,
    "user": {
      "id": 1285421,
      "name": "Xuanlin Mao",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7403245",
    "metrics": {
      "homework_id": "HW11",
      "model_name": "Unknown / Multiple",
      "primary_focus": "model_performance",
      "depth_bucket": "medium",
      "actionability_bucket": "low",
      "word_count": 215
    }
  },
  {
    "id": 7401923,
    "number": 434,
    "course_id": 84647,
    "title": "Special Participation A: Kimi K2 on hw8",
    "document": "I used kimi k2 on hw9 and observed it giving correct answers zero shot on all questions. (even without \"thinking\" tokens) very impressed.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-12-04T09:47:18.097945+11:00",
    "reply_count": 0,
    "view_count": 68,
    "user": {
      "id": 1553113,
      "name": "Nils Selte",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7401923",
    "metrics": {
      "homework_id": "HW8",
      "model_name": "Kimi",
      "primary_focus": "model_performance",
      "depth_bucket": "low",
      "actionability_bucket": "low",
      "word_count": 23
    }
  },
  {
    "id": 7401078,
    "number": 432,
    "course_id": 84647,
    "title": "Special Participation A: Grok on HW 08",
    "document": "I used Grok on the written (non-coding) problems of HW8. It was very strong on the algebraic and conceptual parts (SSM kernels, linear purification, ridge attention), usually getting the correct derivations on the first try. The main issue I saw was in complexity analysis: in a few places it mixed up total work vs. critical path length when discussing parallelization, which required manual correction. Aside from that, the mathematics and reasoning closely matched the official solutions.\n\nFor more details, including annotated interaction logs and comparisons to the official solutions, see the attached PDF.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-04T08:18:32.354006+11:00",
    "reply_count": 0,
    "view_count": 63,
    "user": {
      "id": 1316489,
      "name": "Krish Yadav",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7401078",
    "metrics": {
      "homework_id": "HW8",
      "model_name": "Unknown / Multiple",
      "primary_focus": "model_performance",
      "depth_bucket": "medium",
      "actionability_bucket": "low",
      "word_count": 93
    }
  },
  {
    "id": 7400839,
    "number": 431,
    "course_id": 84647,
    "title": "Special Participation A: Qwen on HW4",
    "document": "For special participation A on HW4, I used Qwen and its Qwen3-Max model with thinking to solve the non coding problems on the homework (problems 1, 2, 3, 4, 7). \n\nI attempted this homework with Qwen 3 times actually as the earlier times I prompted it with more advanced instructions or I would give it each problem seperately it often spiraled and kept second guessing itself. I found that when Qwen does this it often happens cloes to 3 times and it has a hard time accepting the truth when it can't reason to it. This is present in problem 2e in this log as when asking Qwen to reason itself to the solution it took 3 times in one query to finally arrive to the correct answer. \n\nDespite this, Qwen did a pretty good job on this homework especially considering how difficult it is. It interestingly did not one shot computation based questions like the convolution questions in problem 3, but it expertly navigated and explained the matrix calculus in problem 7. I think Qwen is a decent model for this type of tasks but it has to be wrangled more then I would have hoped. \n\nFinal conversation trace: https://chat.qwen.ai/s/5fd54197-ec03-43bd-a4cb-53d560f95d0f?fev=0.1.7",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-04T07:53:08.520193+11:00",
    "reply_count": 0,
    "view_count": 67,
    "user": {
      "id": 961709,
      "name": "Zach Pricz",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7400839",
    "metrics": {
      "homework_id": "HW4",
      "model_name": "Unknown / Multiple",
      "primary_focus": "model_performance",
      "depth_bucket": "medium",
      "actionability_bucket": "medium",
      "word_count": 201
    }
  },
  {
    "id": 7399196,
    "number": 427,
    "course_id": 84647,
    "title": "Special participation A: Kimi on HW0",
    "document": "For Special participation A, I used Kimi on the writing part of HW0. Overall, it was useful, but it also revealed several limitations. Kimi is generally good at recognizing high-level patterns and giving correct final expressions, but it often skips steps, relies too much on memorized formulas, and needs very explicit instructions to produce proper derivations. It also struggles with qualitative reasoning at times.\n\nDespite these issues, Kimi’s conceptual explanations were clear, and with careful prompting and oversight, it was able to provide correct results. In short, Kimi is helpful as long as you guide it closely and verify its reasoning, but it is not reliable for detailed, step-by-step mathematical work without supervision.\n\nHere is the annotated log:",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-12-04T04:34:25.897961+11:00",
    "reply_count": 0,
    "view_count": 60,
    "user": {
      "id": 1751543,
      "name": "ZhaoRui Qu",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7399196",
    "metrics": {
      "homework_id": "HW0",
      "model_name": "Kimi",
      "primary_focus": "model_performance",
      "depth_bucket": "medium",
      "actionability_bucket": "low",
      "word_count": 118
    }
  },
  {
    "id": 7398141,
    "number": 425,
    "course_id": 84647,
    "title": "Special Participation A: Gemini 3 pro on Hw 12",
    "document": "Model Tested: Gemini 3 Pro\n\nOverall Performance: Very good: 100% One-shot\n\nPerformance Overview\n\nThe model was tasked with solving 3 deep learning problems involving debugging neural network initialization, analyzing information theory concepts (KL Divergence behavior), and interpreting Variational Information Bottleneck (VIB) systems.\n\nCode Debugging (Transformers): The model correctly identified a \"peaked softmax\" issue caused by improper weight initialization in a Transformer implementation. It provided the correct theoretical justification (variance scaling) and the exact code fix (Xavier/Glorot scaling) without needing iterative prompting.\n\nMathematical Intuition (KL Divergence): The model successfully generated a counter-example to distinguish Forward vs. Reverse KL divergence properties. It correctly mapped visual plots to \"Mode-Seeking\" vs. \"Mass-Covering\" behaviors based purely on visual evidence and theoretical definitions.\n\nSystem Design (VIB/VAE): The model accurately constructed the computational graph for the Reparameterization Trick and correctly traced gradient flows for encoder/decoder parameters. It also correctly interpreted unlabeled validation error curves by reasoning about the regularization coefficient.\n\nHallucinations & Accuracy\n\nHallucination Rate: 0%.\n\nThe model did not hallucinate. Notably, in Question 3 (Part 2), the model explicitly noted that \"Figure 4 was not included\" but proceeded to solve the problem by deriving what the plots must look like based on VIB theory. This demonstrates a high level of reasoning capability where the model fills missing context with theoretical deduction rather than fabricating visual data.\n\nBehavioral Observations & Strategies\n\nMultimodal Reasoning: The model demonstrated strong vision capabilities, accurately interpreting trend lines in plots (Question 3, Part 1) and distribution shapes (Question 2) without text descriptions of the visual data.\n\nTheoretical Grounding: The model consistently reasoned from first principles. For example, when discussing KL divergence, it didn't just state the answer; it integrated the integral definitions to explain why the support mismatch causes infinity.\n\nStrategy - Contextual Chunking: For Question 3, which was long and multi-part, I broke the prompt into two segments (Parts a/b, then c/d). This likely helped the model maintain focus, though its strong performance suggests it might have handled the full context in one go.\n\nConclusion\n\nGemini 3 Pro demonstrated sophisticated understanding of deep learning theory, effectively bridging the gap between mathematical notation, code implementation, and visual interpretation of training dynamics. It functioned as a highly competent tutor, correctly solving all non-coding theoretical components one-shot.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-04T01:08:18.606596+11:00",
    "reply_count": 0,
    "view_count": 55,
    "user": {
      "id": 622765,
      "name": "Gabriel Han",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7398141",
    "metrics": {
      "homework_id": "HW12",
      "model_name": "Unknown / Multiple",
      "primary_focus": "model_performance",
      "depth_bucket": "medium",
      "actionability_bucket": "medium",
      "word_count": 373
    }
  },
  {
    "id": 7397817,
    "number": 422,
    "course_id": 84647,
    "title": "Special Participation A: ChatGPT o3 on HW 9",
    "document": "Executive Summary\n\nFor Homework 9 (all but prob 5), I worked through the written parts with ChatGPT to see how well it could solve the questions on the first try, how often it drifted or hallucinated, and how solid its reasoning was. For most of the assignment, especially Questions 1 through 4, it did surprisingly well. It handled the computation-heavy parts, the code-completion questions, and the conceptual attention explanations almost exactly the way the official solutions did. Its derivations for expectations, variances, argmax attention, and the multi-head attention code were all clean and matched the intended logic with basically no backtracking.\n\nOne thing that stood out was how consistently the model could jump straight into the right structure of the problem. It didn’t need much nudging to set up sums, identify independence assumptions, or translate equations into PyTorch einsums. Even the complexity analysis in Question 4 lined up with the official answers once we made sure the symbols matched. When it was right, it was very right, and the explanations were easy to follow.\n\nThe main issue came up in Question 6. ChatGPT fell into a pretty common trap: it used the homogeneous quadratic kernel (q transpose k squared) instead of the standard degree-2 polynomial kernel ((q transpose k plus 1) squared). Because of that, its first attempt at the feature map was missing the constant term and the linear terms, so that part drifted from the correct answer. But once I asked it to revisit its reasoning instead of giving a new solution, it immediately spotted the mistake, explained why the earlier map didn’t match the actual kernel, and rebuilt everything correctly. So the error wasn’t really a hallucination—more like it defaulted to a familiar formula without checking whether it matched what the homework meant.\n\nAside from that one slip, the model didn’t really hallucinate or spiral. It stayed structured and patient even in the longer reasoning chains, and it didn’t show any of the “I’m stuck so I’m going to guess something wild” behavior LLMs sometimes fall into. When it got something wrong, it was usually because it applied the wrong convention, not because it made something up.\n\nOverall, ChatGPT handled the written problems well. It can one-shot most derivations, and when it does mess up, it’s usually easy to steer it back on track with a focused prompt. The combination of clear explanations, good algebraic intuition, and willingness to self-correct makes it a pretty solid tool for these kinds of homework questions.\n\nBelow is the annotated log:",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-12-03T20:45:21.592344+11:00",
    "reply_count": 0,
    "view_count": 47,
    "user": {
      "id": 647361,
      "name": "Tamzid Razzaque",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7397817",
    "metrics": {
      "homework_id": "HW9",
      "model_name": "Unknown / Multiple",
      "primary_focus": "model_performance",
      "depth_bucket": "medium",
      "actionability_bucket": "medium",
      "word_count": 420
    }
  },
  {
    "id": 7397298,
    "number": 419,
    "course_id": 84647,
    "title": "Special Participation A: Hw2 with Gemini Pro 3 Thinking Mode",
    "document": "For the special participation A on HW2, I use Grok to address the non-coding analytical problems 1, 2 and 7. Gemini did it great as expected, all questions were correct on the first-shot prompt. I used images as the prompt instead of the direct text.\n\nSummary: Gemini 3 is one of the models for math questions and it doesn't disappoint, every procedure was at least acceptable, understandable and most important correct (doesn't hallucinate with these questions). The answers were correct and also using different notations.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-03T17:16:38.165773+11:00",
    "reply_count": 0,
    "view_count": 71,
    "user": {
      "id": 1770933,
      "name": "Gustavo Jose Ortiz Zepeda",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7397298",
    "metrics": {
      "homework_id": "HW2",
      "model_name": "Unknown / Multiple",
      "primary_focus": "model_performance",
      "depth_bucket": "low",
      "actionability_bucket": "low",
      "word_count": 85
    }
  },
  {
    "id": 7397226,
    "number": 417,
    "course_id": 84647,
    "title": "Special Participation A: Gemini 3 pro on HW 8",
    "document": "I used Gemini 3 Pro to solve the non‑coding portion of HW 8. Overall, Gemini did an excellent job producing clear and well‑structured mathematical derivations, often matching the logical flow of the official solutions. I especially appreciated its ability to organize the reasoning into intuitive steps and explain the purpose behind each transformation. Gemini sometimes took a slightly different path to reach the final answer, but its results were fully consistent after minor prompting. Overall, I was impressed by its precision, presentation quality, and the clarity of its mathematical reasoning.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-12-03T16:58:24.482712+11:00",
    "reply_count": 0,
    "view_count": 72,
    "user": {
      "id": 1751466,
      "name": "Tin Yau",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7397226",
    "metrics": {
      "homework_id": "HW8",
      "model_name": "Unknown / Multiple",
      "primary_focus": "model_performance",
      "depth_bucket": "medium",
      "actionability_bucket": "low",
      "word_count": 90
    }
  },
  {
    "id": 7397166,
    "number": 416,
    "course_id": 84647,
    "title": "Special Participation A: Gemini 3 Pro on HW 2 Written Questions",
    "document": "Executive Summary: Interaction with Gemini on Deep Learning Theory\n\nModel Tested: Gemini 3 Pro\n\nDomain: Deep Learning Optimization & Distributed Training (CS 182/282 Context)\n\nOverall Performance: 100% Success Rate (One-shot)\n\nPerformance Overview\n\nThe model was tasked with solving non-coding theoretical problems involving:\n\nOptimization Derivations: Deriving analytical solutions for penalized linear improvement using Euclidean ($L_2$) and Infinity ($L_\\infty$) norms.\n\nOptimizer Convergence: Analyzing convergence points for Vanilla SGD, Simplified Adam, and Feature Rescaling on a constrained linear regression problem.\n\nDistributed Training: Calculating communication costs (message count and size) for All-to-All, Parameter Server, and Ring All-Reduce architectures.\n\nIn every instance, the model provided the correct analytical solution and numerical answers on the first attempt (one-shot). No prompt engineering, iterative correction, or \"dragging\" of the model was required to arrive at the solution. The model successfully transcribed complex mathematical notation directly from uploaded screenshots without OCR errors.\n\nHallucinations & Accuracy\n\nHallucination Rate: 0%.\n\nThe model demonstrated robust reasoning capabilities. It correctly identified standard optimizers (Gradient Descent, SignSGD) from first-principles derivations and accurately recalled specific distributed systems constraints (e.g., the $2(n-1)$ steps in Ring All-Reduce).\n\nBehavioral Observations: Uncertainty and Verification\n\nA distinct behavioral trait observed during the session was the model's tendency to simulate self-verification and hedging.\n\n\"Double-Checking\": Despite the high accuracy of the final outputs, the model’s reasoning process appeared to involve recursive checks. It would often derive a step, then implicitly verify it against known standard results (e.g., checking if the derived update rule matched the standard definition of SignSGD) before committing to the answer.\n\nHedging Language: The model frequently used probabilistic language such as \"I believe\" or \"this suggests,\" rather than authoritative absolutes, even when the math was unambiguous. This behavior mimics a cautious human student double-checking their work to ensure logical consistency, rather than a machine simply outputting a retrieved token sequence.\n\nConclusion\n\nGemini demonstrated graduate-level competency in deep learning theory, capable of handling multimodal inputs (LaTeX screenshots) and complex analytical derivations with perfect accuracy. The interaction suggests that for well-defined theoretical problems, modern reasoning models can serve as reliable verification engines, provided the user monitors the \"reasoning track\" for the model's internal consistency.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-03T16:47:42.131618+11:00",
    "reply_count": 0,
    "view_count": 65,
    "user": {
      "id": 915218,
      "name": "Ijin Yu",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7397166",
    "metrics": {
      "homework_id": "HW2",
      "model_name": "Unknown / Multiple",
      "primary_focus": "model_performance",
      "depth_bucket": "medium",
      "actionability_bucket": "medium",
      "word_count": 353
    }
  },
  {
    "id": 7393256,
    "number": 399,
    "course_id": 84647,
    "title": "Special Participation A: Claude (Sonnet 4.5) on HW 12",
    "document": "Below is my report on using Claude's Sonnet 4.5 model to solve the written questions to Homework 12. I have also attached a PDF of the annotated transcript.\n\nSummary:\n\nOverall Claude correctly one-shots every question\n\nGenerally, it's reasoning is correct, except for one slightly informal/imprecise statement that it makes about distributions in the second problem.\n\nIn general, I felt that its explanations lacked pedagogical value and would not be maximally helpful to a student who was confused about the class material",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-03T08:38:40.394007+11:00",
    "reply_count": 0,
    "view_count": 99,
    "user": {
      "id": 923022,
      "name": "Ishir Garg",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7393256",
    "metrics": {
      "homework_id": "HW12",
      "model_name": "Claude",
      "primary_focus": "model_performance",
      "depth_bucket": "medium",
      "actionability_bucket": "medium",
      "word_count": 81
    }
  },
  {
    "id": 7389909,
    "number": 397,
    "course_id": 84647,
    "title": "Special Participation A: Gemma 3 (12b params) on HW09 Written Problems",
    "document": "Executive Summary:\n\nI worked with Gemma 3 on the written problems for Homework 9 (problems 1, 2, 3, 4, and 6). Overall, Gemma performed fairly well (especially given that it is an open-source model that I ran locally on my laptop). For problems 1–4e, which were largely computation problems, Gemma (mostly) produced correct solutions on the first attempt and consistently demonstrated a strong grasp of the underlying concepts.\n\nOne area where Gemma stood out was its clarity of explanation. Compared to ChatGPT and Claude (the other LLMs I have worked with), I've found Gemma's explanations to be clearer, or explicit, and pedagogically helpful. This, in part, is because Gemma uses more English in their mathematical argument, whereas (in my experience) ChatGPT and Claude tend to be equation-heavy. Gemma's strong explanations (when correct) make it a particularly helpful learning tool, even when it gets some problems wrong. I actually found myself better understanding some of the time and space-complexity arguments around attention mechanisms when trying to guide Gemma to the right solution.\n\nGemma’s weaknesses emerged in two specific areas:\n1: time- and space-complexity analysis\n\n2: dimension checking and tensor shape reasoning.\n\n\nHowever, even when it produced incorrect bounds or incorrect tensor shapes, it often identified the correct overall strategy, so using Gemma can still be instructive. \n\nOne of the more striking behaviors appeared toward the end. After many failed attempts and struggling through problems 4f, 4g, and 6, Gemma seemed to exhibit an emergent pattern resembling human frustration. Its responses became less coherent, more speculative, and more willing to guess just to produce something and move on. I found this shift in behavior similar to how a tired human might respond when stuck on a difficult p-set. When Gemma enters this state, the best strategy is simply to start a new conversation for higher-quality answers (however, I continued in the same conversation out of curiosity).\n\nOverall, I found Gemma 3 to be an instructive assistant for the written problems. \n\nTrace:\nhighlighted themes - clarity of explanations, where Gemma 3 became lost, and emergent \"frustrated\" behavior.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-02T20:19:39.414415+11:00",
    "reply_count": 0,
    "view_count": 68,
    "user": {
      "id": 635294,
      "name": "Etaash Patel",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7389909",
    "metrics": {
      "homework_id": "HW9",
      "model_name": "Unknown / Multiple",
      "primary_focus": "model_performance",
      "depth_bucket": "medium",
      "actionability_bucket": "low",
      "word_count": 344
    }
  },
  {
    "id": 7389325,
    "number": 392,
    "course_id": 84647,
    "title": "Special Participation A: Gemini 3 Pro on HW 7",
    "document": "I used Gemini 3 Pro to solve the non-coding portion of HW 7, which include 3(b), 4, 7, and 8. Overall, Gemini 3 Pro did a quite good job of giving an intuitive explanation for some of the complex math found in the official HW solutions. It was able to one-shot almost all the questions (except one part in question 4 about the blog). Gemini did usually take quite different approaches than the official solution, which made me prompt Gemini further to connect the different solutions together. I do believe that Gemini could have been more rigorous for some of the more math-intensive questions, but I didn't expect to get something fully similar to the official solution without giving more context on what math techniques to use. Overall, I am quite impressed with the results.\n\nHere is an annotated trace of the chat: https://docs.google.com/document/d/1mXvXyKigfz0mgcQdqG-h3eDCklwMNgAnccCIv5bq4uk/edit?usp=sharing",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-02T16:53:58.575623+11:00",
    "reply_count": 0,
    "view_count": 73,
    "user": {
      "id": 647490,
      "name": "Vrushank Prakash",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7389325",
    "metrics": {
      "homework_id": "HW7",
      "model_name": "Unknown / Multiple",
      "primary_focus": "model_performance",
      "depth_bucket": "low",
      "actionability_bucket": "medium",
      "word_count": 144
    }
  },
  {
    "id": 7386904,
    "number": 384,
    "course_id": 84647,
    "title": "Special Participation A: Mistral on HW 1",
    "document": "I have used Mistral to test on Hw 1. \n\nLink: https://chat.mistral.ai/chat/6ff004cd-66c9-49ef-92fb-19476f51402b\n\nSummary:\n\nIn general, it was able to get most of the answers without any mistakes. A lot of the work shown by the LLM followed the solutions of what the course has provided. Looking at the solution that Mistral provided, it seems like a lot of the computational mistakes are coming from matrix calculations. For example, it was able to correctly reason 3a however it failed to get 3b even though, from 3a to 3b, it is just simple matrix calculations. Since question 3 is built upon previous questions, since it messed up on 3b, it messes up on the following subpart questions for 3. \n\nIt seems like the mathematical reasoning is good since the non-matrix computations are all working very well without any errors. I think it also got better when I pointed out that there are some computational errors because after that there has not been any mistakes. \n\nI think the one-shot for Mistral has about 80% accuracy on the problems. I tried to ask Mistral to fix some of the mistakes  that it has made on the previous answer, but it seems to misunderstand my request and always go with the same incorrect question. It seems to be that MIstral is \"self-centered\" or cannot identify the questions that it has been asked.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-02T11:24:16.403925+11:00",
    "reply_count": 0,
    "view_count": 45,
    "user": {
      "id": 227880,
      "name": "Minjune Kim",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7386904",
    "metrics": {
      "homework_id": "HW1",
      "model_name": "LLM (unspecified)",
      "primary_focus": "model_performance",
      "depth_bucket": "medium",
      "actionability_bucket": "low",
      "word_count": 226
    }
  },
  {
    "id": 7382863,
    "number": 376,
    "course_id": 84647,
    "title": "Special Participation A: Mistral AI's Le Chat on HW5 Written Portion",
    "document": "For Special Participation A, I used Mistral AI's Le Chat on the written portion of HW5. Overall, it performed averagely - mostly arriving at correct answers in one shot, but in a couple of cases requiring further prompting to correct mistakes. I found some indication of a lack of ability to refer to chat history in it's responses.\n\nHere is the link to my chat: https://chat.mistral.ai/chat/9c9c7fc7-7985-45a9-b7cd-df337fec5d26",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-02T02:48:01.810497+11:00",
    "reply_count": 0,
    "view_count": 39,
    "user": {
      "id": 647373,
      "name": "Kian Hekmatnejad",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7382863",
    "metrics": {
      "homework_id": "HW5",
      "model_name": "Unknown / Multiple",
      "primary_focus": "model_performance",
      "depth_bucket": "low",
      "actionability_bucket": "low",
      "word_count": 66
    }
  },
  {
    "id": 7381174,
    "number": 368,
    "course_id": 84647,
    "title": "Special Participation A: Qwen on HW7",
    "document": "I used Qwen3-Max model to solve the written parts of homework 7. Instead of feeding individual questions with full text, I first uploaded the entire homework PDF and then asked each sub-question only by its label (“7(b)”, “8(a)”, etc.). The goal was to see whether Qwen could (1) avoid forgetting earlier context, (2) correctly retrieve the appropriate problem statement from the embedded PDF, and (3) solve each part in a single attempt. In this experiment Qwen successfully accomplished (1), (2) but failed (3) for questions that require SVD math. \n\nOne-shot accuracy: ~70%. Qwen performs well for questions that require basic understanding of classic models like encoder-decoder, multiple choice questions, and blog summary. \nHallucinations: None observed\nModel failures: Qwen tends to fail questions with sophisticated mathematical operations like SVD, linear algebra. It tends to stick to the notation in the question and is reluctant to apply more advanced tricks that simplify the expression, but rather to choose to brute force derive. In addition, Qwen might interpret wording like baseline with its own definition (updated baseline rather than benchmark), which might cause confusion if not clearly stated. \nContext retention: Surprisingly strong—Qwen consistently located the correct question inside the PDF without needing me to restate it.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-01T15:15:40.022846+11:00",
    "reply_count": 0,
    "view_count": 41,
    "user": {
      "id": 699069,
      "name": "Ruihan Xia",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7381174",
    "metrics": {
      "homework_id": "HW7",
      "model_name": "Unknown / Multiple",
      "primary_focus": "model_performance",
      "depth_bucket": "medium",
      "actionability_bucket": "medium",
      "word_count": 203
    }
  },
  {
    "id": 7380526,
    "number": 367,
    "course_id": 84647,
    "title": "Special Participation A: Using Gemini Flash 2.5 on HW11",
    "document": "Trace: https://gemini.google.com/share/2e206d7da648\n\nMath + T/F Question Zero-Shot Performance: 13/15.\n\nPrompt Structure: I told the model that it was a DL assistant for me. I gave the model the concepts I was planning to ask it about, and let the model know to output its interpretation of the question, followed by a step-by-step solution and a final answer for each. I also told the model that it should be able to correct itself when needed. For each question, I simply provided images of screenshots of the homework. For some questions, I explicitly told the model what context to consider, when needed. \n\nAnalysis: The model starts off a bit slow on the very first question (about ideas to adjust LoRA to get better performance). It identifies the main idea correctly, but does not get two of the other ideas from the solutions, even after re-prompting. Following this somewhat slow start, the model goes on a long streak of answering questions thoroughly and correctly. The solutions given by the model are arguably better than the actual HW solutions in some cases. In others, the model is potentially not concise enough, but this is also partially due to my initial prompt, where I asked the model to go step-by-step and then provide a final answer (sort of a CoT way of prompting the model).\n\nInterestingly, towards the end, the model starts trying to reattempt questions that it had already solved. For example, when I prompted Gemini to do 6a, it initially tried to resolve question 1 which it had already completed. When I asked it do then do 6a, it initially got the question slightly wrong due to some sort of apparent reading issue (I provided it with an image that it seemed to mis-extract the text of). This signified that Gemini 2.5 Flash may be better leveraged for individual questions, rather than for the entire HW assignment, as context length seems to be an issue here. It did the same thing when asked to solve 6b with an image, as it tried to then solve problem 2. On 6c, it actually got two of the T/F questions wrong. Even when re-prompted with the context again (just in case this was the cause once again, as with 6a), it got them wrong. It seems that the performance of the model gets better and then gets worse, empirically.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-01T13:04:21.853163+11:00",
    "reply_count": 0,
    "view_count": 50,
    "user": {
      "id": 622883,
      "name": "Aaryan Chandna",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7380526",
    "metrics": {
      "homework_id": "HW11",
      "model_name": "Unknown / Multiple",
      "primary_focus": "model_performance",
      "depth_bucket": "medium",
      "actionability_bucket": "medium",
      "word_count": 393
    }
  },
  {
    "id": 7377516,
    "number": 364,
    "course_id": 84647,
    "title": "Special Participation A: Kimi K2 (Thinking) on HW1",
    "document": "Model Used: Kimi k2\n\nAssignment: Homework 1 (Non-coding theoretical problems)\n\nOverview\n\nI utilized Kimi k2 to solve the theoretical portions of Homework 1, covering topics from gradient descent stability and momentum dynamics to stochastic differential equations and high-dimensional geometry. The model demonstrated exceptional performance, acting as a highly competent graduate-level assistant. It successfully derived complex proofs and stability conditions that matched the instructor's ground truth almost perfectly.\n\nThe prompt I used is pretty simple, such as \"Solve Q1, show me all your process that leads to your answer\".\n\nPerformance Metrics & Observations\n\nOne-Shot Accuracy: High (>90%).\n\nThe model required very little \"hand-holding.\" For complex multi-part questions (like Question 2 on SVD coordinates and Question 3 on Momentum eigenvalues), Kimi k2 maintained context across subsections without needing to be reminded of previous definitions. It correctly identified the underlying mathematical structures (e.g., using the SVD to diagonalize the loss landscape) immediately upon being presented with the problem statement.\n\nMathematical Rigor & Reasoning:\n\nUnlike some LLMs that skip steps or hallucinate intermediate lines to reach a \"known\" answer, Kimi k2 provided complete, step-by-step derivations. E.g.\n\nMatrix Calculus: It correctly handled vector derivatives in Question 6 (Tikhonov Regularization) and Question 7 (MAP interpretation), distinguishing properly between scalar and vector layouts.\n\nStability Analysis: In Question 3, it correctly applied the stability criterion (checking if roots lie within the unit circle) to the characteristic equation, a non-trivial task for standard language models.\n\nHallucinations/Misconceptions: Little Observed.\n\nThe model did not invent theorems or misuse standard notation. In instances where the problem allowed for multiple interpretations (e.g., the specific formulation of Adam in Question 4), it defaulted to the standard textbook definitions that aligned with the course material.\n\nDownside: Kimi K2 generally have a thinking time of over 1 minute on each question. This is outperformed by other models, such as GPT5 or Gemini 3. However, this thinking time provides great ultimate performance that renders it fruitful. \n\nConclusion\n\nKimi k2 proved to be an incredibly effective tool for theoretical deep learning. It did not merely \"retrieve\" answers but seemingly \"reasoned\" through the linear algebra and probability theory required for this assignment. Its ability to link algebraic manipulations (like rotating the basis Vtop) to geometric intuitions makes it a powerful study aid for understanding the \"why\" behind deep learning dynamics.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-11-30T18:25:52.153833+11:00",
    "reply_count": 0,
    "view_count": 46,
    "user": {
      "id": 941542,
      "name": "Hanyang Gu",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7377516",
    "metrics": {
      "homework_id": "HW1",
      "model_name": "Unknown / Multiple",
      "primary_focus": "model_performance",
      "depth_bucket": "medium",
      "actionability_bucket": "low",
      "word_count": 381
    }
  },
  {
    "id": 7377431,
    "number": 362,
    "course_id": 84647,
    "title": "Special Participation A: Deepseek on HW9",
    "document": "Intro:\n\nThis is an attempt to interact with deepseek on non-coding parts of homework 9. The purpose of this study is to better understand how to prompt/interact with LLMs more effectively and LLM’s capability of solving real life reasoning/math related problems with few-shot prompting. The specific model I interacted with was DeepSeek-V3.2. I used Deepseek’s web ui to interact with the model. I will focus on how different prompting methods (or modes of reasoning) affect a model's one-shot correctness of the problems, and how to improve its accuracy without providing more in context examples. \n\n\n\nConversation traces:\n\nhttps://chat.deepseek.com/share/jkzqnyn7j8say9v7jc\nhttps://chat.deepseek.com/share/838vauzbwa2g0ynfby\nhttps://chat.deepseek.com/share/pp0exea4mnmt36qfqu\n\n\n\nReport with fully annotated traces:\n\n\nSummary:\n\nTo sum up, the model (DeepSeek-V3.2) is a quite strong model and is able to solve most of the non coding questions correctly (one-shot). For this specific problem set, two things matter the most for accuracy: 1. Model’s reasoning ability and 2. Correct prompt and context. Prompt and context are very important because if instructions and context are not explicitly told, the model will make certain assumptions, and thus give incorrect answers. For example in question 4, when not stated explicitly, the model thinks it is completing some code function instead of filling blanks for a written question, thus identifying the wrong blanks to fill. The model’s reasoning ability is also very important : for the previous example, even if the correct context and prompt is provided, the model still was not able to identify the correct blanks to fill without the DeepThink feature on. Together, these observations suggest that while DeepSeek-V3.2 is already quite capable on non-coding questions, its performance is highly sensitive to both how we phrase the task and model’s reasoning capabilities. Thus, carefully designing prompts and systematically leveraging features like DeepThink will be essential for reliably getting correct answers and understanding the model's core strengths and weaknesses.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-11-30T17:33:46.197807+11:00",
    "reply_count": 0,
    "view_count": 48,
    "user": {
      "id": 509429,
      "name": "Alex Cao",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7377431",
    "metrics": {
      "homework_id": "HW9",
      "model_name": "Unknown / Multiple",
      "primary_focus": "model_performance",
      "depth_bucket": "medium",
      "actionability_bucket": "medium",
      "word_count": 306
    }
  },
  {
    "id": 7375514,
    "number": 354,
    "course_id": 84647,
    "title": "Special Participation A: Gemini (Fast) on HW 9",
    "document": "Executive Summary:\n\nI used Gemini on the non-coding parts of HW 9, and evaluated where it did well and where it didn't. I noticed Gemini could mostly one-shot these questions, especially sections that were very structured math or required analyzing code. I think it mainly failed where the formatting of the question was an issue, and that was just because of LaTeX copy/paste error. To solve these, when I clarified the misreads, Gemini quickly fixed its solution. Even with the formatting problems though, when it was a coding analysis question or there was a lot of context, Gemini still did very well, one shotting almost all the questions. I think the only case where the solution was incorrect showed inconsistency within its own solution, providing the incorrect answer as the header and then showing work to derive the correct answer. This was confusing, but it ultimately did come up with the correct solution. I was also impressed with how quickly it was able to come up with the solutions, taking no longer than 10 seconds at the max. This could also be, however, that I fed each problem in individually, allowing it to see all context for each question as it came up. \n\nI also noticed Gemini provided detailed explanations for each question, serving as a great conceptual recap on the topic. It helped me understand how different concepts were related, and it helped me understand derivations for formulas that I previously didn't know. I think overall, it helped me with my understanding of course concepts a lot. \n\nOne thing I did find a little annoying was Gemini wouldn't let me paste the code I copied from the hw9 pdf file: I needed to paste the question in a different document, and then copy that over to the Gemini search bar. I think maybe Gemini wasn't used to the LaTeX pdf format, and didn't allow those characters. This did make it a little harder to do, but it was just a mild inconvenience!\n\nHere is a link to my annotated logs: https://drive.google.com/file/d/11Kqd0IzU7LoCcbPa70mZT0KVR2X1V1Ql/view?usp=sharing",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-11-30T06:29:30.10322+11:00",
    "reply_count": 0,
    "view_count": 33,
    "user": {
      "id": 608657,
      "name": "Divya Ramesh",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7375514",
    "metrics": {
      "homework_id": "HW9",
      "model_name": "Unknown / Multiple",
      "primary_focus": "model_performance",
      "depth_bucket": "medium",
      "actionability_bucket": "medium",
      "word_count": 342
    }
  },
  {
    "id": 7374016,
    "number": 348,
    "course_id": 84647,
    "title": "Special Participation A: Gemini 2.5 Flash on HW0",
    "document": "Hey guys — I used Gemini Flash 2.5 for the non-coding parts of HW0, and here’s the quick verdict.\n\nTL;DR: Gemini was strong on the “mechanical” math (clean chain rule + indicator notation, solid ridge/SVD manipulations, decent linear-algebra bookkeeping), but its main weakness was qualitative/sign reasoning in the ReLU elbow SGD question, where it overclaimed elbow direction and bungled sign/inequality logic. \n\nWhat it did well: used the 1ϕ(x)>0​ indicator instead of messy piecewise cases, and matched the staff-style update notation w′,b′,e′. \n\n\nWhere it slipped: for case (ii), it said “elbow shifts right,” but the elbow can move left or right depending on bias + step size. \n\n\nClear wrong answer: for (iii) it concluded the elbow “moves left,” but your annotation points out the expression is negative so becoming “less negative” means it actually moves right. \n\n\nEven when it got the final direction right: its proof sketch ended with a bogus condition like w>bx.\n\nAfter I nudged it with an “algebra-first + sanity-check” checklist (explicitly compute e′=−b′/w′, be careful when w<0, and verify with a tiny numeric example), Gemini improved a lot. It stopped hand-waving the geometry, caught its own sign mistakes when the numeric check contradicted the algebra, and finally gave consistent conclusions across the cases. The conclusion: Gemini is super useful here, but only if you force it to prove elbow motion by comparing e′ vs e and validating with a quick counterexample.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-11-29T12:21:31.421792+11:00",
    "reply_count": 0,
    "view_count": 36,
    "user": {
      "id": 1316386,
      "name": "Jason Trinh",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7374016",
    "metrics": {
      "homework_id": "HW0",
      "model_name": "Unknown / Multiple",
      "primary_focus": "model_performance",
      "depth_bucket": "medium",
      "actionability_bucket": "medium",
      "word_count": 234
    }
  },
  {
    "id": 7373861,
    "number": 346,
    "course_id": 84647,
    "title": "Special Participation A: Gemini-Pro 3 on HW9",
    "document": "I used Gemini-pro 3 on HW 9 and it performed very well. The main issues with Gemini-pro 3 was the small details in arithmetics that needed correction especially in linear algebra and time/space complexity; however, the solutions were overwhelmingly correct.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-11-29T11:05:16.255596+11:00",
    "reply_count": 0,
    "view_count": 35,
    "user": {
      "id": 892816,
      "name": "Shervin Goudarzi",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7373861",
    "metrics": {
      "homework_id": "HW9",
      "model_name": "Unknown / Multiple",
      "primary_focus": "model_performance",
      "depth_bucket": "low",
      "actionability_bucket": "low",
      "word_count": 40
    }
  },
  {
    "id": 7372448,
    "number": 345,
    "course_id": 84647,
    "title": "Special Participation A: Deepseek on Hw8",
    "document": "For HW8, I first provided Deepseek with a set of instructions to encourage step-by-step reasoning and self-verification. For each problem, I supplied both the image of the question and a direct copy-and-paste text version. After observing its behavior throughout the assignment, I summarized several notable characteristics:\n\n1. Its self-checking mechanism was largely ineffective.\n Although Deepseek always performed a “self-examination” step, it rarely identified actual mistakes. Most of the time, it simply reiterated the correctness of its own answer rather than performing a thorough or systematic review of potential oversights.\n\n2. It occasionally misinterprets small details in the prompt, though its overall understanding remains accurate.\n These misunderstandings were usually minor (e.g., subtle assumptions or edge cases), but they indicate that Deepseek may overlook fine-grained nuances in certain problem statements.\n\n3. Its reasoning and explanatory abilities are very strong.\n Deepseek is well-suited for explaining solution steps and clarifying concepts. Since HW8 did not contain many heavy calculations, its computational accuracy appeared reliable in this context.\n\n4. Its conversational and correction capabilities are adequate, but its error-localization ability is weak.\n When its solution was incorrect, Deepseek could fix the issue once I explicitly pointed out the problem. However, it generally struggled to independently locate the exact source of the error without guidance.\n\nOverall, Deepseek performs well as a conceptual explanation tool but still requires human intervention for precise error detection and critical verification. Attached below is a PDF of my conversation with deepseek.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-11-28T21:00:50.226819+11:00",
    "reply_count": 0,
    "view_count": 60,
    "user": {
      "id": 1751552,
      "name": "Zesheng Cai",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7372448",
    "metrics": {
      "homework_id": "HW8",
      "model_name": "DeepSeek",
      "primary_focus": "model_performance",
      "depth_bucket": "medium",
      "actionability_bucket": "medium",
      "word_count": 240
    }
  },
  {
    "id": 7372081,
    "number": 339,
    "course_id": 84647,
    "title": "Special Participation A: Deepseek on HW2",
    "document": "I completed all non-coding parts of Homework 2 using Deepseek with DeepThink turned on.\n\nMotivation\n\nI’ve seen quite a few examples of classmates engaging in some light conversation with the model or using somewhat structured/designed prompts with the aim of improving the output quality and accuracy. I opted for a slightly different approach. I decided to use raw and \"no-prompt\" prompting, meant to mimic the setting where a student is treating the LLM less as an interactive tutor and more as a tool, a “calculator”, for problems. \n\nFor every problem, my process consisted of highlighting the relevant subpart in its entirety on the homework PDF, then copy pasting into the text box. Nothing more. No context was given except for those present in the problem originally, and no prompt optimization was performed (i.e., no strategic \"filler\" words like \"think carefully and answer accurately\", \"you are an expert in deep learning\", \"take a deep breath\" [https://arxiv.org/abs/2309.03409] were included).\n\nSummary\n\nOverall, Deepseek is a very capable model for reasoning tasks. It one-shots every non coding question in Homework 2. It arguably fails to notice one small detail in one of the subparts (see Q1 for more), but apart from that, all perfect. \n\nAnalyzing the thinking trace reveals Deepseek's general chain of thought pattern, which is comprised of these steps:\n\nRestate the question\n\nRe-interpret the question with its own words\n\nList what is given/known\n\nIdentify what is unknown/what the task is\n\nFormulate a strategy/plan of attack\n\nCarry out that strategy (i.e., start doing the work/calculations following the strategy)\n\nAfter finding answer, start doubting absolutely everything starting from step 1 to step 6.\n\nFor every doubt, recheck work and see if other answers exist. If yes, explore those paths; else, move on to the next doubt.\n\nReturn final answer after double-checking everything.\n\nPeople who took EECS 16A with Prof. Ranade (Fall 2023) might remember her teaching us a three step proof writing structure: \n\nWhat do we know?\n\nWhat are we trying to show?\n\nHow can we get from 1 to 2?\n\nIt is interesting to see Deepseek's thought habits closely follow this three step process (plus a plethora of double-checking), almost like Deepseek's frontier-level reasoning performance stems from the fact that it treats every technical question like completing a rigorous proof. \n\nPlease see annotations for more in-depth observations.\n\nFiles & Links\n\nQuestion 1:\n\nQuestion 2:\n\nQuestion 5:",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-11-28T14:45:16.624977+11:00",
    "reply_count": 0,
    "view_count": 242,
    "user": {
      "id": 961796,
      "name": "Ken Zheng",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7372081",
    "metrics": {
      "homework_id": "HW2",
      "model_name": "Unknown / Multiple",
      "primary_focus": "model_performance",
      "depth_bucket": "medium",
      "actionability_bucket": "low",
      "word_count": 393
    }
  },
  {
    "id": 7358125,
    "number": 315,
    "course_id": 84647,
    "title": "Special Participation A: Grok on HW 9",
    "document": "Hey guys! I worked with Grok to solve all of the non-coding parts of homework 9, and here are the results.\n\nTL;DR - Grok was generally very successful at solving all parts of this transformers homework, in which a major focus was keeping track of the dimensions of various matrices (key, query, value) and determining time complexities of operations. To this end, Grok integrated existing knowledge about transformer architectures and their interpretations, which streamlined the solution process for some problems but occasionally included added assumptions that were inconsistent with the problem statement. Overall, Grok could one-shot each question with some prompting, with Question 6 as an exception where a few clarifications were required.\n\n\n\nProblem 1:\n\nFor this problem, I initially set the stage by establishing Grok in its role and providing it with the problem PDF directly from the course website. Throughout the process, I was impressed by Grok's ability to parse information and recall details from the problem set throughout our solution process for all parts. To help with this recall, I intentionally repeat question numbers in my prompt and describe the adjacent subject matter of the question, which I hoped would help it localize to the correct regions of context for each problem. In this first case, it solved 2 of the 3 subparts with ease, and could immediately revise its answer for the third after a gentle prod about data types (it initially gave the answer as the square of a vector mu, rather than the norm squared). The conversation for Problem 1 is as follows:\n\nProblem 2:\n\nThis problem went even more smoothly than problem 1. Once again providing the question number, addressing specific part numbers, and providing a high-level characterization of the problem's subject matter, I was able to get Grok to one-shot this highly conceptual problem.\n\nProblem 3:\n\nI was initially concerned that Grok might have trouble parsing the PDF formatting surrounding fill-in-the-blank code, but I shouldn't have worried. Grok was able to easily one-shot the fill-in-the-blank code.\n\nProblem 4:\n\nGrok aces it again! As usual, I made sure to indicate the problem number and qualitatively describe the tasks Grok needed to perform before having it attempt the question. In one area it determines a time complexity of O(bnd/h), but in its solution process mentions that this is equivalent to the solutions answer of O(bnk).\n\nProblem 6:\n\nUnlike the rest of the problems, Grok had more trouble ironing out the finer details. For example, it took a little prompting to adjust coefficients in its kernelization (despite verbally accounting for cross-term symmetry, it did not initially provide the corresponding scaling factor) and to correctly transpose certain matrices (rather than following the notation of the homework, it decided to transpose all of the matrices per other notation it had seen previously). In the first instance of hallucination throughout the session, it insisted that M = D was defined in Equation (2), which simply did not happen. As a result, I needed to ask it the hypothetical of 'What if M and D were distinct values?\" in order to elicit the desired response.\n\nFor your reference, here is a PDF of the entire chat log. Of all of the questions on this homework, Grok actually made the least sense discussing its \"Homework Process and Study Group\". :)\n\nThanks guys. Have a wonderful day!",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-11-24T22:25:02.317067+11:00",
    "reply_count": 0,
    "view_count": 84,
    "user": {
      "id": 622810,
      "name": "Jaimyn Drake",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7358125",
    "metrics": {
      "homework_id": "HW9",
      "model_name": "Unknown / Multiple",
      "primary_focus": "model_performance",
      "depth_bucket": "medium",
      "actionability_bucket": "medium",
      "word_count": 556
    }
  },
  {
    "id": 7357397,
    "number": 314,
    "course_id": 84647,
    "title": "Special Participation A: Gemini (Thinking With Pro 3) on HW06",
    "document": "I used Gemini (Thinking with Pro 3) to solve every non-coding question of homework 6.\n\nHere's the PDF summarizing our interaction:\n\nOverview of Performance\n\nGemini acted as a \"Teaching Assistant/Technical Solver\" under well-defined rules I designed to reduce hallucination and maximize precision. The model successfully solved 100% of the non-coding questions, 9/13 in the first attempt, and the remaining 4/13 on the second attempt after additional guidance or clarification from me. Gemini demonstrated strong reasoning capabilities in mathematical proofs (GNN path counting) and theoretical analysis (Muon optimizer scaling), but required additional prompting for visual interpretation tasks (understanding tables and graph figures) and producing nicely-formatted outputs. \n\nOutcomes\n\nOne-Shot Success Rate: ~70%\n\nHigh success: Mathematical derivations, standard Deep Learning theory (memory calculations, optimizer comparisons), text-based conceptual questions.\n\nLower success: Questions requiring visual extraction from the PDF (e.g., graph topology in Figure 3) or fill-in-the-blank tables with loose formatting.\n\nHallucinations: 0 major hallucinations.\n\nInterestingly, when solving question 3ciii, the model did not hallucinate graph edges when they were missing from the text extraction; instead, it paused and requested me to describe the topology. I believe it did this because I said it could ask me for clarification if it was confused about anything in my initial prompt.\n\nMisconceptions: Minor. Initially provided a generic answer for a specific \"analogy table\" question (Q2f) and needed a nudge to be more thorough in connecting molecular facts to graph inputs (Q2e).\n\nNeat Observation: Gemini Meta-cognition\n\nGemini surprisingly took the initiative to write a Python script to help it read a table in the homework PDF when its built-in PDF processing subroutine failed to correctly parse it.\n\nIt explained that my \"Hallucination Check\" and \"Precision\" rules provided it motivation to develop and run this script to help it solve the errors it detected.\n\nNote that I did not tell Gemini to consider writing any code. Gemini decided to do this all on its own. \n\nHere are excerpts of what Gemini explained to me:\n\nYou asked about my decision to write a Python script to parse the PDF. My reasoning was driven by the \"Hallucination Check\" and \"Precision\" protocols you established.\n\n1. In my initial scan, I noticed the prompt for Question 2(f) asked me to \"Fill out the rest of the table.\" However, standard text extraction often garbles table formatting, making it impossible to distinguish between row headers and cell content. I could not see the \"empty\" cells I was supposed to fill.\n\n2.  Rather than guessing the structure or hallucinating the missing rows based on general knowledge, I needed the exact raw text layout to identify which GNN concepts were missing corresponding to the CNN concepts.\n\n3. I wrote the script to dump the text surrounding \"Question 2\" to ensure I had the specific headers (e.g., \"Color jitter\", \"Image flip\") to construct the correct analogies.\n\nGemini explained how it decides what to do for regular interactions:\n\nThink of writing a Python script as requiring high \"activation energy.\" It takes tokens, time, and computational effort.\n\n> Standard Prompt: \"Help me with homework.\" -> Low urgency. I preserve energy and use my internal weights.\n\nIn my standard \"default\" mode, my primary directive is usually conversational fluidity and helpfulness. When I encounter a PDF where a table is slightly garbled: \n\n> Path of Least Resistance: I often try to \"error correct\" using my internal knowledge base. I might look at the words \"CNN\" and \"GNN\" and simply guess what the analogy should be based on general Deep Learning knowledge. \n\n> The Result: I would likely have given you a plausible-sounding answer that was actually hallucinated (not based on the specific PDF text).\n\nGemini noted that my prompt's constraints were a deciding factor in guiding its decisions because they forced it to change its alignment and incentive structure for our interaction to prioritize avoiding hallucinations:\n\nTo be completely honest: Without your specific protocols, it is highly unlikely I would have written that Python script.\n\n> Your Prompt: \"Rigorous,\" \"High Academic Integrity,\" \"Self-Correction.\" -> High urgency. The \"cost\" of being wrong became higher than the \"cost\" of writing and executing code.\n\nYour instruction to perform a \"Hallucination Check\" and strictly \"Verify\" acted as a hard constraint—a \"stop loss\" on my behavior. Because I was forbidden from guessing (Hallucination Check) and forbidden from giving up (Technical Solver persona), I was forced to find a third path. That third path was using the code tool to parse the file differently. \n\nIn summary: You gave me the incentive structure that made writing the code the only acceptable solution. You successfully \"cornered\" me into being smart.\n\nThis shows that Gemini has a sophisticated instruction-following ability and can perform tasks resembling meta-cognition and recognize hallucinations in its own thinking.\n\n\n\nEdit: Added links for the archive\n\nPersonal website: https://nraultwang.github.io/\n\nGithub: https://github.com/nraultwang",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-11-24T15:41:59.195499+11:00",
    "reply_count": 0,
    "view_count": 114,
    "user": {
      "id": 582199,
      "name": "Nicolas Rault-Wang",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7357397",
    "metrics": {
      "homework_id": "HW6",
      "model_name": "Unknown / Multiple",
      "primary_focus": "model_performance",
      "depth_bucket": "high",
      "actionability_bucket": "high",
      "word_count": 793
    }
  },
  {
    "id": 7353572,
    "number": 309,
    "course_id": 84647,
    "title": "Special Participation A: GPT-5 HW4",
    "document": "Generally. GPT-5 generates accurate answers for conceptual and computation questions. There are some conventions that it chooses to use that we don't use in class, like Xavier initialization using 1/sqrt(d). Some questions also required some further prompting, though generally one-shot is quite accurate.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-11-23T15:18:18.637808+11:00",
    "reply_count": 0,
    "view_count": 193,
    "user": {
      "id": 961740,
      "name": "Nyx Iskandar",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7353572",
    "metrics": {
      "homework_id": "HW4",
      "model_name": "Unknown / Multiple",
      "primary_focus": "prompting_strategy",
      "depth_bucket": "low",
      "actionability_bucket": "low",
      "word_count": 43
    }
  },
  {
    "id": 7353091,
    "number": 308,
    "course_id": 84647,
    "title": "Special Participation A: Claude Sonnet 4.5 on HW3",
    "document": "Below is my report on using Claude's Sonnet 4.5 model to solve the written questions to Homework 3. I have also provided a link to the original conversation I had with Claude. Question-specific comments can be found in the PDF.\n\nFormatted PDF of conversation and additional comments:\n\nRaw Conversation:\nhttps://claude.ai/share/9305bd53-16e3-423c-b2de-143974dab634\n\nOverall Summary:\n\nAccuracy: Claude was able to get all but 2 of the questions correct on the first try. The questions it got incorrect were Q1b and Q5b. For Q1b, it made an error when doing some math calculations, and for Q5b, it did not consider loading the activations for layers 5 and 10. Despite these mistakes, it only took an additional comment from me to steer the model to the correct response.\n\nExplanations: Claude made their process of solving the questions very clear, and in my opinion explained its answers better than the answer key. A notable case of this is Q4c, where Claude identifies the need to use the chain rule to solve the question, which the answer key glosses over since the gradient of mu in that problem was just the identity matrix and didn't affect the final result.\n\nLength: Claude responded with very long answers all things considered. While they were quite long, I didn't find anything too extraneous in the answers. The most I would see of Claude giving more details about a specific answer it provided (e.g. explaining that the policy gradient was used in the REINFORCE algorithm. I also found Claude's extra information very useful for my understanding in some cases, particularly when it explained the papers from Q3.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-11-23T12:23:45.008857+11:00",
    "reply_count": 0,
    "view_count": 162,
    "user": {
      "id": 961870,
      "name": "E Harrison",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7353091",
    "metrics": {
      "homework_id": "HW3",
      "model_name": "Claude",
      "primary_focus": "model_performance",
      "depth_bucket": "medium",
      "actionability_bucket": "medium",
      "word_count": 266
    }
  },
  {
    "id": 7335374,
    "number": 300,
    "course_id": 84647,
    "title": "Special Participation A: Kimi on HW5",
    "document": "Above is the chat log. I found Kimi K2 w/ Thinking enabled to be able to one-shot 90% of the questions on the non-coding parts of HW5. This surprisingly included the ability to one shot questions that required images – for example, choosing whether the given image represented batch norm or layer norm it was able to correctly respond. The only error it ran into was when it hallucinated the next part of the question a bit and went off on a tangent about \"covariance\" even though it wasn't really mentioned in the question. I had to manually correct it by reminding it of the question and formatting it in plain text. With this information, it had interesting thought traces in its chain of thought, such as:\n\n\"The user is correcting me about part (b) of the question. Let me re-read the prompt carefully.\"\n\n\"This might be asking about Cov(x_i, y_i) or something else. But the user's clarification is clear: they want the gradient analysis. I should focus on what the user is asking for now, not what the original prompt might have intended.\"\n\nThis was interesting to me because it was able to correct itself and keep itself on-course. I also found that the reasoning traces often included pretty granular steps involved for calculating derivatives and working through more mechanical calculations such as the manual convolution calculation in the first question. These granular steps probably allow it to make incremental progress where each incremental step is in-distribution.\n\nI honestly didn't have much strategy because it was able to one-shot so many of the questions. Overall, I found that short prompts work better as to not pollute the context window. Restarting chats for new questions also helped to not have it get confused by older parts.\n\nAnother observation I had was that most times it included a lot of extra fluff around the final answer and even answered some things that were not asked to go above and beyond. This could be helpful for understanding and learning but probably not so much if your main goal is to just solve the problem because now you must dig through the response to find it.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-11-19T18:19:41.009736+11:00",
    "reply_count": 0,
    "view_count": 45,
    "user": {
      "id": 622831,
      "name": "Kabir Shah",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7335374",
    "metrics": {
      "homework_id": "HW5",
      "model_name": "Kimi",
      "primary_focus": "model_performance",
      "depth_bucket": "medium",
      "actionability_bucket": "medium",
      "word_count": 363
    }
  },
  {
    "id": 7322058,
    "number": 286,
    "course_id": 84647,
    "title": "Special Participation A: Qwen on HW8",
    "document": "Below is my report on using Qwen3-Max on the written part of homework 8.\n\nOverall, I'm very impressed by Qwen3-Max's performance on this homework. It seems like it really does well on the questions where it needs to fill in or do multiple choice, i. e., problems 3 and 4. The main issues I had were in the second half of problem 1, which was regarding computational efficiency; it had a hard time accurately responding in text along with giving a mathematical reasoning.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-11-17T18:22:18.14388+11:00",
    "reply_count": 0,
    "view_count": 199,
    "user": {
      "id": 1751558,
      "name": "Hanna Roed",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7322058",
    "metrics": {
      "homework_id": "HW8",
      "model_name": "Unknown / Multiple",
      "primary_focus": "model_performance",
      "depth_bucket": "medium",
      "actionability_bucket": "low",
      "word_count": 83
    }
  },
  {
    "id": 7315986,
    "number": 283,
    "course_id": 84647,
    "title": "Special Participation A: Deepseek on HW6",
    "document": "I used DeepSeek on HW 6 non coding questions and here is my annotated log file with reflections.\nhttps://drive.google.com/file/d/1XfIWHSvILyZ-1hMksxc6cLJF1uHOxsKb/view?usp=sharing\n\nTL;DR DeepSeek can one-shot most conceptual, math-heavy parts (roughly 70–80% of subparts) with answers very close to the official solution. Its mistakes are structured, not random hallucinations: mainly convention/spec mismatches (row vs column, “no analog needed” parts), and graph/figure misreads.\nMy enforced structure (Restated Problem → Plan → Reasoning → Self-Check) makes its thinking transparent but does not make it truly self-critical. It rarely catches its own deeper mistakes.\n\nReflection\nUsing DeepSeek on the non-coding parts of HW6, I found that it followed my structured prompt very well: it restated each question, proposed a plan, walked through step-by-step reasoning, and ended with a self-check. This made its reasoning transparent and easy to annotate. On many subparts, it produced answers that were essentially identical to the official solution on the first try. However, there were two clear failure modes. The first was convention mismatch: for example, in Q2(c) it understood that updates correspond to multiplying by the adjacency matrix, but it hedged between left- and right-multiplication instead of committing to the “rows → multiply on the left” convention that the homework fixes. The second failure mode was misreading the graph: in Q3(c)(iii), it simply used the wrong neighbor sets for nodes 2 and 3, so the final formulas were incorrect despite having the right functional form.\n\nWhat I find interesting is the “Self-Check” sections mostly checked basic sanity and whether the answer addressed the question, but they did not help the model catch these deeper issues. It rarely expressed uncertainty, even in places where the mapping was clearly ambiguous or dependent on the figure. Overall, I think DeepSeek can get you most of the way to a good solution with clear reasoning, but you cannot safely copy its answers blindly. To reach fully correct solutions, you still need to actively verify conventions and adjust answers.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-11-16T11:39:27.836513+11:00",
    "reply_count": 0,
    "view_count": 145,
    "user": {
      "id": 1762358,
      "name": "Angelina Zhang",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7315986",
    "metrics": {
      "homework_id": "HW6",
      "model_name": "DeepSeek",
      "primary_focus": "model_performance",
      "depth_bucket": "medium",
      "actionability_bucket": "low",
      "word_count": 323
    }
  },
  {
    "id": 7307445,
    "number": 279,
    "course_id": 84647,
    "title": "Special Participation A: Mistral AI on HW0 written",
    "document": "Sorry for being so late. I used Le Chat to assist with the conceptual and mathematical parts of homework, specifically the ReLU/SGD analysis and vector calculus derivations. The experience was overwhelmingly positive, with the assistant demonstrating strong expertise in linear algebra, optimization, and neural network dynamics.\n\nStrengths:\n\n1. Le Chat correctly derived most of the results.\n\n2. The assistant provided clear, structured derivations for the optimization, SVD, and MAP estimation perspectives, making complex concepts (like the pseudoinverse and Woodbury identity) accessible.\n\nLimitations:\n\nWhile the assistant excelled at algebraic manipulation, it could not directly parse or reason about visual elements (e.g., graph structures or plots). For example, if the problem had included a diagram of the ReLU function or a computational graph, I had to describe it textually for accurate analysis.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-11-14T12:56:44.53714+11:00",
    "reply_count": 2,
    "view_count": 166,
    "user": {
      "id": 1751464,
      "name": "Tianqu He",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7307445",
    "metrics": {
      "homework_id": "HW0",
      "model_name": "Unknown / Multiple",
      "primary_focus": "model_performance",
      "depth_bucket": "medium",
      "actionability_bucket": "medium",
      "word_count": 130
    }
  },
  {
    "id": 7302906,
    "number": 277,
    "course_id": 84647,
    "title": "Special Participation A: Qwen on HW9",
    "document": "For the special participation A on HW9, I use Qwen to solve the non-coding analytical components (problems 2–5). The performance was very strong -- almost all questions were very quickly solved by directly copy-pasting the question, which had multiple parts often.\n\n\n\nExecutive Summary\n\nUsed qwen thinking, with max thinking context length (81920 tokens).\n\n I also noticed from other attempts that often giving the hint associated with the question (if the question has a hint) generally helps make its answer more likely to be correct, or guides it in the right direction very well.\n\n For Question 3 part b, I had to nudge it to give the answer regarding the new out_features value. However, after reminding it, all of its answers were correct.\n\n I also ignored small formatting issues, and noticed even with formatting issues (some symbols were not pasted/displayed properly), Qwen was still able to understand all the questions and provide correct solutions.\n\n For longer questions (such as question 6), I also thought about pasting/asking the questions in chunks, rather than the entire multi-part question at a time. However, I found that Qwen was able to answer correctly even when giving the entire question at a time.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-11-13T15:59:58.033068+11:00",
    "reply_count": 0,
    "view_count": 145,
    "user": {
      "id": 959003,
      "name": "Oliver Chen",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7302906",
    "metrics": {
      "homework_id": "HW9",
      "model_name": "Unknown / Multiple",
      "primary_focus": "model_performance",
      "depth_bucket": "low",
      "actionability_bucket": "low",
      "word_count": 197
    }
  },
  {
    "id": 7297480,
    "number": 275,
    "course_id": 84647,
    "title": "Special Participation A: Gemini 2.5 Pro on HW5",
    "document": "I used Gemini 2.5 Pro to solve the written parts of Homework 5 (Q1-4). I started by mentioning that I wanted to solve a problem set related to a specific topic (in this case, basics of CNNs) and went over the four problems one by one with attached screenshots of the entire problem. I’ve attached an annotated pdf of my interactions with the LLM and a link to my original chat. \n\nIn summary, even without extensive context in my typed prompt, Gemini 2.5 Pro was able to extract information included within the problem from the screenshots themselves and one-shot solve almost all the problems correctly without any hallucinations. \n\nNotable observations: \n\nGood parsing of information - Most of the time it correctly parsed all mathematical equations, figures and text from the screenshots, even when a problem was spread across multiple screenshots. This is evident because it often types back key information from the problem statement to formulate the answer. All final answers (after corrections noted below) had no mathematical or conceptual errors. \n\nMisinterpretation of a hint - In question 1b), it used the hint to solve the problem itself. I wonder if this is because it mostly encountered instances of math problems where the hint is used to solve the problem itself. However, when I went through the other special participation threads for HW5 (that were posted as of 11/12/25), I noticed that none of those LLMs used the hint this way and often correctly interpreted it. So, it was interesting to see this behavior from Gemini. When I pointed this misinterpretation to Gemini, it acknowledged the mistake and solved the problem correctly using substitutions for the system of equations. \n\nOverly complicated solutions - In this homework I would like to point out to its answers for the subparts in question 2b) in the attached pdf. While none of the approaches were incorrect, there were steps that were unnecessary to build the solution (annotated in the pdf). \n\nIncomplete reasoning - The answer to 4b) could have been more complete. One could see that from the first term we can derive the transformation we need to apply to $\\check{w}$ to get to equation (3) (as the LLM did). However, I think the answer would be complete if it mentioned what should also happen to the $\\Gamma$ matrix (even though this was not explicitly asked in the question) for the equation to achieve the form in equation (3) rather than treating it as something obvious. \n\nIn conclusion, I think Gemini 2.5 Pro is fully capable of answering all questions correctly with good mathematical and conceptual reasoning. However, some of the solution approaches could be more concise and complete.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-11-12T20:46:02.16884+11:00",
    "reply_count": 0,
    "view_count": 128,
    "user": {
      "id": 963909,
      "name": "Kithmini Herath",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7297480",
    "metrics": {
      "homework_id": "HW5",
      "model_name": "LLM (unspecified)",
      "primary_focus": "model_performance",
      "depth_bucket": "medium",
      "actionability_bucket": "medium",
      "word_count": 445
    }
  },
  {
    "id": 7283953,
    "number": 262,
    "course_id": 84647,
    "title": "Special Participation A: ChatGPT on HW6",
    "document": "I used ChatGPT (5) on the non-coding parts of homework 6 (#2, #3). I prompted it by attaching the entire homework pdf and asking it to answer them as an expert in deep learning. In its first response, it was able to one-shot a majority of the questions. Some of its solutions, however, contained imprecise language. For example, in 2d, it just described how the max function worked, so I had to convince it that it would be a one-hot encoding by induction. This was interesting since it seemed like the question contained enough context for this to be concluded. Another interesting limitation was that ChatGPT was unable to parse the image of the graph for 3b. Even after I screenshotted the graph and fed it as input again, it got one of the connections wrong. Overall, I would say ChatGPT did extremely well on this homework, requiring only a few clarifications, especially for visual aspects.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-11-10T13:51:11.522627+11:00",
    "reply_count": 0,
    "view_count": 148,
    "user": {
      "id": 591910,
      "name": "Jameson Liu",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7283953",
    "metrics": {
      "homework_id": "HW6",
      "model_name": "ChatGPT (other)",
      "primary_focus": "model_performance",
      "depth_bucket": "low",
      "actionability_bucket": "medium",
      "word_count": 156
    }
  },
  {
    "id": 7267427,
    "number": 245,
    "course_id": 84647,
    "title": "Special Participation A: Claude on HW2 written part",
    "document": "Claude demonstrates strong mathematical reasoning capabilities and correctly derived analytical solutions without any mathematical hallucinations or false claims. Across all problems, Claude never makes computational errors. The issues are always at the level of solution strategy or conceptual completeness, not arithmetic or algebra. This suggests the model's mathematical symbolic reasoning is quite robust, while its ability to choose optimal solution paths requires more guidance. The two problems requiring guidance (Problem 1b and Problem 2a) revealed a consistent pattern: Claude tends to solve problems in an over-complicated way before recognizing simpler approaches. When I used some questioning like “Are you sure...?\" and quoting Claude's own observations back to it as a hint, Claude is able to quickly find cleaner solutions and even provided further insights compared to the original homework solution.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-11-07T04:30:53.053726+11:00",
    "reply_count": 0,
    "view_count": 126,
    "user": {
      "id": 1155278,
      "name": "Yaqi Su",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7267427",
    "metrics": {
      "homework_id": "HW2",
      "model_name": "Claude",
      "primary_focus": "model_performance",
      "depth_bucket": "medium",
      "actionability_bucket": "medium",
      "word_count": 130
    }
  },
  {
    "id": 7266065,
    "number": 244,
    "course_id": 84647,
    "title": "Special Participation A: Mistral on HW2",
    "document": "I tested Mistral on the non-coding parts of HW2\n\nChat history link: https://chat.mistral.ai/chat/678d9106-0d96-45c6-83e1-2c0ac7a7384a\n\nAnnotated Log: \n\nExecutive Summary:\n\nI found that it could one-shot 2(a) and Question 5. The former was a simple conceptual subquestion, while the latter involved reasoning about distributed training and computational scaling. Mistral performs well when the task relies on text understanding, structural reasoning, or recalling standard frameworks, but it struggles with problems that require original mathematical derivation or deeper logical adaptation.\n\nThroughout the interaction, I noticed that Mistral often exhibited what I’d call lazy reasoning: it tried to map every prompt to a familiar textbook pattern rather than reasoning from the specific assumptions of the problem. Even when I pointed out inconsistencies or paradoxes, it tended to defend its original, polished-sounding explanation until I explicitly instructed it to ignore existing results and reason under the given setup. Only then did it converge to the correct logic.\n\nOverall, Mistral shows strong linguistic fluency and confidence, but limited flexibility and self-correction. It can one-shot straightforward or pattern-based questions, yet it fails to generalize to novel problem settings without direct intervention.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-11-06T18:22:12.750338+11:00",
    "reply_count": 0,
    "view_count": 98,
    "user": {
      "id": 1294618,
      "name": "Xi Cheng",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7266065",
    "metrics": {
      "homework_id": "HW2",
      "model_name": "Unknown / Multiple",
      "primary_focus": "model_performance",
      "depth_bucket": "medium",
      "actionability_bucket": "medium",
      "word_count": 182
    }
  },
  {
    "id": 7265693,
    "number": 243,
    "course_id": 84647,
    "title": "Special Participation A: Gemini on Homework 4",
    "document": "Annotated Transcript:\n\nhttps://drive.google.com/file/d/1ZOIMXval6EtWYyoBE6H13fS0I7d58Fmd/view?usp=sharing\n\nFor this special participation, I used Gemini Pro 2.5 to solve the written portions of homework 4. I began by giving Gemini the assignment and telling it that I was trying to evaluate how well it could solve the problems, and then went through the problems with it one by one. Overall, it did a good job of solving the problems. \n\nAll the mistakes it made, except one, were due to the fact that it read the problems in the homework wrong. Oftentimes, these misreadings were very blatant, like when it just misread the entries of the matrices for problem 3. It only made one reasoning mistake, that wasn’t due to it misreading the question, in part 2g, but was able to correct itself after being told what step in the derivation it made a mistake on. \n\nOne notable part of the interaction was when it solved problem 2e. Its answer differed from what was given in the solutions, even though it was correct as there seems to be a mistake in the solutions, so I tried prompting it to fix its answer, at which point it arrived at the same solution. Afterwards, I gave it what was given in the solutions, and asked it to try to arrive at the answer in the solutions. After trying to arrive at the given answer, it eventually gave up, and said that it didn’t know why the answer was correct. I thought that this was pretty impressive because it shows that Gemini is actually critiquing itself as it goes. Instead of hallucinating to match the answer I gave it, it pushed back and gave what it thought was correct. I thought this was really impressive, as it isn’t just being “agreeable” and taking what the prompter says to be the truth, like other LLMs I’ve used like ChatGPT.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-11-06T16:19:43.889302+11:00",
    "reply_count": 0,
    "view_count": 122,
    "user": {
      "id": 647625,
      "name": "Jason Guo",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7265693",
    "metrics": {
      "homework_id": "HW4",
      "model_name": "Unknown / Multiple",
      "primary_focus": "model_performance",
      "depth_bucket": "medium",
      "actionability_bucket": "medium",
      "word_count": 309
    }
  },
  {
    "id": 7263386,
    "number": 239,
    "course_id": 84647,
    "title": "Special Participation A: HW6, gpt-oss-120b",
    "document": "I used gpt-oss-120b with thinking on HW 6 non coding questions. The performance was surprisingly good for a open-source model from a company with flagship proprietary models. It was able to one-shot almost all the questions. This model was also surprisingly fast (although that depends on the hosting provider). The latency was near 0 seconds and token generation was very fast. A more detailed summary is provided in the pdf.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-11-06T10:05:54.778256+11:00",
    "reply_count": 0,
    "view_count": 123,
    "user": {
      "id": 617558,
      "name": "Alex Luu",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7263386",
    "metrics": {
      "homework_id": "HW6",
      "model_name": "Unknown / Multiple",
      "primary_focus": "mixed/other",
      "depth_bucket": "medium",
      "actionability_bucket": "low",
      "word_count": 70
    }
  },
  {
    "id": 7259824,
    "number": 234,
    "course_id": 84647,
    "title": "Participation Section A: Llama (Meta AI) on HW0",
    "document": "I interacted with Llama/Meta AI and had a rather disappointing result. For the (easier) questions it answered, it made decent progress on showing the math steps but was vague at times and couldn't tell left multiplication from right. It had trouble answering questions one-by-one like a student would, instead fixating on the topic of the question and ranting about it.",
    "category": "Admin",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-11-05T21:02:05.590684+11:00",
    "reply_count": 0,
    "view_count": 118,
    "user": {
      "id": 69192,
      "name": "Shashwat Bansal",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7259824",
    "metrics": {
      "homework_id": "HW0",
      "model_name": "LLaMA",
      "primary_focus": "meta_reflection",
      "depth_bucket": "low",
      "actionability_bucket": "medium",
      "word_count": 60
    }
  },
  {
    "id": 7258633,
    "number": 230,
    "course_id": 84647,
    "title": "Special Participation A: Kimi on HW7 Written Questions",
    "document": "I had Moonshot AI's Kimi K2 model answer HW7's written questions. For each problem, my goal was to get the model to the correct answer while giving it as little outside assistance as possible. I only attempted to steer the model when it gave a clearly incorrect answer or when its response diverged significantly from the staff solution.\n\nHere are some of my key findings:\n\nThe model was able to answer most questions in one shot. Laying aside minor formatting and notational differences, the final answers were almost always the same as the staff solution's. \n\nHowever, in reasoning through the problems, the model sometimes made logical leaps that, while correct, were not sufficiently justified in my opinion.\n\nFor example, for Q3b, the model goes straight from stating the loss function to computing its gradient WRT $W_2$ in just one step.\n\n(For context in the below equation, $\\hat{X} = W_2 W_1 X$)\n\nMeanwhile, the staff solution is much more thorough, walking us through every step of the derivation and explicitly citing the matrix calculus identities that were employed.\n\nSo to fix that issue, when necessary, I ask the model to justify in more detail how it reached a given step.\n\nI saw some evidence of hallucination; for question 4a, I accidentally forgot to give the model some information needed to answer the question correctly. But rather than state, \"not enough information provided\", it gave me an incorrect response.\n\nFurthermore, when I gave it a URL containing the information needed to answer the question correctly, it again answered the question wrong. \n\nUpon further questioning, the model revealed that in fact, it does not have live browsing capabilities.\n\nThat is a reasonable limitation, but what is concerning is that it didn't admit it couldn't read the URL until after it had already hallucinated. This decreases my overall trust in the model.\n\nThat said, the model did a pretty good job on the \"summarize a blog post\" question (apart from the hallucination).\n\nI feel like these sorts of \"summarize a technical blog\" questions are where LLMs' abilities really shine. \n\nThey are able to pick out the key details and \"big picture\" insights without getting too lost in the technical details (a mistake that I often make when summarizing blog posts / papers)\n\nA full, annotated transcript of the conversation can be found here: https://drive.google.com/file/d/1p6f0AXpDIMW-9v7vZMJVj2J7aqjPkWQe/view?usp=sharing. \n\nI will also add a comment to this post with screenshots of the conversation to make it easy to read.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-11-05T14:34:52.365835+11:00",
    "reply_count": 1,
    "view_count": 110,
    "user": {
      "id": 921495,
      "name": "Vijay Kethanaboyina",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7258633",
    "metrics": {
      "homework_id": "HW7",
      "model_name": "Unknown / Multiple",
      "primary_focus": "model_performance",
      "depth_bucket": "medium",
      "actionability_bucket": "medium",
      "word_count": 410
    }
  },
  {
    "id": 7250623,
    "number": 226,
    "course_id": 84647,
    "title": "Special Participation A: HW7 with Grok",
    "document": "I used Grok to complete the written part of HW7. I begin by clearly stating Grok’s role and the assistance I require, then provide whole HW7 file to Grok and ask it to understand the problem setups ONLY, so it does not skip ahead to solving the problem.\n\nHW7 consists of proofs, multiple choice questions, and open-ended free response problems. Grok performs very well on the multiple-choice questions, often answering correctly on the first attempt. For the open-ended free response problems, Grok provides reasonable points with clear explanations. However, on the proof problems, Grok typically does not match the official solution on the first attempt and requires additional hints and guidance, for example, specifying which variables or equations to use.\n\n\n\nOverall Grok performs descently on the written part of the homework, with great performance on MCQ problems and open-ended free response problems, but difficulty with the proofs.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-11-04T09:47:19.742032+11:00",
    "reply_count": 0,
    "view_count": 123,
    "user": {
      "id": 647575,
      "name": "Ender Ji",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7250623",
    "metrics": {
      "homework_id": "HW7",
      "model_name": "Unknown / Multiple",
      "primary_focus": "model_performance",
      "depth_bucket": "low",
      "actionability_bucket": "low",
      "word_count": 147
    }
  },
  {
    "id": 7250482,
    "number": 225,
    "course_id": 84647,
    "title": "Special Participation A: Mistral on HW6",
    "document": "Link: https://chat.mistral.ai/chat/6cd62931-f284-4c4b-9ba3-c5b97943fd28\n\nAnnotated Log: https://drive.google.com/file/d/1mUgllouQGxVA_m5tmlVrq6Dt46HSf0hd/view?usp=sharing\n\nExecutive Summary:\n\nI observed that the model fails to extract information from diagrams like tables, images, and graphs. Assignments that include filling out information or extracting information from diagrams to complete the assignment are very likely to be incorrect, as the model will hallucinate information.\n\nAlso, it didn't list the explanation for its reasoning behind its own analysis of the graphs in one of the problems. I suggest being careful with problems involving visualization.\n\nIt did well in demonstrating its mathematical analysis, making it clear to figure out any mishap in its derivation. There were times when the model outputted answers that I, myself, hadn't seen, though they were mathematically equivalent. The model considers other options when solving problems, allowing for a broader understanding of mathematics.\n\nMistral also did a good job of allowing for an open interaction with the user by making sure to end with, ‘Ready for any further questions or clarification!’",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-11-04T09:28:36.310964+11:00",
    "reply_count": 0,
    "view_count": 103,
    "user": {
      "id": 970764,
      "name": "Heidy Hernandez Juan",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7250482",
    "metrics": {
      "homework_id": "HW6",
      "model_name": "Unknown / Multiple",
      "primary_focus": "model_performance",
      "depth_bucket": "medium",
      "actionability_bucket": "medium",
      "word_count": 159
    }
  },
  {
    "id": 7250444,
    "number": 224,
    "course_id": 84647,
    "title": "Special Participation A: Gemini Pro on HW 3",
    "document": "What I did:\n\nI ran an interactive, non-coding walkthrough of HW 3 using Gemini Pro. Full trace (screenshots + chat excerpts) is in my doc: https://docs.google.com/document/d/1P6yTAFO4GR4W4a_l02kAFN2mYwKtmgsmPLecVg9gAGY/edit?tab=t.0\n\nI used Gemini to sanity-check my derivations, clarify notation, and cross-validate with my own solutions\n\nWorkflow:\n\n1) Try solo first (≤30 min). If blocked, paste the exact sub-question into Gemini.\n\n2) Compare & probe. Check Gemini’s output against my derivation; ask for clarifications or for the missing step.\n\n3) Tighten. Clarify equation statements, symbol mismatches, and ask for the key identity or rules explicitly.\n\nSpecific Example (Analogy + Clarification):\n\nTopic: Interpreting pθ​:X→Δ(X) in Q4.\n\nMy Ask: “What does the arrow to Δ(X) mean in pθ​:X→Δ(X)?”\n\nGemini’s Analogy (helpful): Sound equalizer.\n\npθ​ = the equalizer; θ = slider positions;\n\nX = the set of audible frequencies;\n\nΔ(X) = all possible sound profiles (distributions over X).\n Changing θ selects a particular distribution in Δ(X).\n\nMy Follow-up (to make it precise): “Please restate without analogy: define X, Δ(X), and what an ‘element’ of Δ(X) is.”\n\nGemini (clarified): X is the sample space; Δ(X) is the set of all probability measures on X; for each θ, pθ​(⋅)∈Δ(X).\n\nPattern used: take the first pass (analogy/intuition), then demand the formal mapping with symbols.\n\nObservations:\n\nUses analogies to simplify (e.g., equalizer for pθ​:X→Δ(X)). Helpful as a first pass.\n\nInconsistent symbols at times, likely from generic training patterns; needs nudging to match the homework question’s notation.\n\nPositives (What Worked Well):\n\nOne-shot on standard rewrites: Correctly gave correct answers straight from around 30% of the questions\n\nRight methodology even when imperfect: When wrong/incomplete, it still pointed in the correct direction\n\nNegatives (Limitations and Effort Required):\n\nNotation slippage: Misread or drifted symbols\n\nOver/under-explanation: Wrapped easy steps in prose but skimmed tough steps unless I asked “elaborate on this step.”\n\nPrompting Strategies That Helped:\n\n“Derive further.” Ask for the exact algebraic substitution (e.g., show ∇p=p∇logp).\n\n“Elaborate / Why this?” Request the chain-rule Jacobian or the gradient wrt input vs. parameters explicitly.\n\n“Is my thought correct? If not, why?” Good for catching subtle mistakes and forcing a counterexample or fix.\n\n“Show this equation more clearly” Keeps it concise and reduces misunderstanding\n\n“Match the HW notation.” Prevents variable chaos.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-11-04T09:23:05.224039+11:00",
    "reply_count": 2,
    "view_count": 142,
    "user": {
      "id": 906474,
      "name": "Ben Yu",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7250444",
    "metrics": {
      "homework_id": "HW3",
      "model_name": "Unknown / Multiple",
      "primary_focus": "model_performance",
      "depth_bucket": "medium",
      "actionability_bucket": "low",
      "word_count": 362
    }
  },
  {
    "id": 7246769,
    "number": 219,
    "course_id": 84647,
    "title": "Special Participation A: ChatGPT on HW7",
    "document": "I evaluated ChatGPT‑5's capabilities for HW7 non‑coding parts: 3(b), 4, 7, 8, using the hw7 questions and staff‑solutions as ground truth.\n\nBottom line. ChatGPT‑5’s answers are correct. \n\nPer‑problem highlights.\n\n3(b) PCA & linear autoencoders. Derived the first‑order conditions exactly as in the key—∇W2​​L=2(W2​W1​−I)XX⊤W1⊤​, ∇W1​​L=2W2⊤​(W2​W1​−I)XX⊤—and correctly verified that W2​=Uk​,W1​=Uk⊤​ satisfies them. Verdict: correct. \n\n4 “How to train your ResNet.” Reported 341 s to 94% baseline and 26 s to ≈94.1% final; reflections (b, c) are on‑point (data‑pipeline bottlenecks, batch size/LR scaling, BN precision, “catastrophic forgetting” vs curvature). Verdict: correct (with extra but accurate context). \n\n7 Machine translation. (a) Correctly explains why “vertical stacking” is flawed (breaks variable‑length handling and global conditioning). (b) Teacher forcing tokens enumerated explicitly: <SOS>, I, see, a, dog. (c) Evaluation: <SOS>, then model’s outputs (I, saw, a, dog). Verdict: correct. \n\n8 Self‑supervised linear AEs. (a) Exactly matches the objective: two Linear layers (encoder/decoder), MSELoss, SGD + weight decay; no Dropout/LN/BN. (b) Clear SVD argument that the λ‑regularized optimum favors orthonormal columns in W2​ (minimizing σ2+1/σ2 at σ=1). Verdict: correct. \n\nOverall assessment. The model’s responses align with the staff solutions in substance and notation, with small stylistic differences only.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-11-03T18:16:17.084933+11:00",
    "reply_count": 0,
    "view_count": 118,
    "user": {
      "id": 607238,
      "name": "Faiaz Khan",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7246769",
    "metrics": {
      "homework_id": "HW7",
      "model_name": "ChatGPT (other)",
      "primary_focus": "model_performance",
      "depth_bucket": "low",
      "actionability_bucket": "low",
      "word_count": 191
    }
  },
  {
    "id": 7244375,
    "number": 217,
    "course_id": 84647,
    "title": "Special Participation A: Gemini 2.5 Flash on HW2",
    "document": "I interactively engaged Gemini 2.5 Flash on the non-coding parts of Homework 2. Overall, the model was able to arrive at the correct answers in most cases, though several notable issues were observed.\n\nStrategies: I first clarified the main role that Gemini was expected to perform and illustrated the evaluation rubrics for its answers. Then, I do the following steps:\n\nStep 1: Provide Gemini with one homework problem.\n\nStep 2: Collect its initial (one-shot) answer.\n\nStep 3: If the answer was incorrect, provide hints to guide Gemini to fix the original answer, and repeat until the response was correct or it seems to have no chance of fixing it.\n\nStep 4: Repeat Steps 1–3 for all homework problems.\n\nNotable observations and flaws. Although Gemini was able to correctly solve most of the questions in the first answer, it occasionally made logical or mathematical errors that led to incorrect final answers, even when the reasoning process is flawless. In some cases, the model relied on specific examples provided in the prompt, which resulted in partially correct answers. Additionally, Gemini sometimes produced wrong-formed LaTeX code, making its output less readable. This issue may have been partly influenced by the formatting of the input prompts.\n\nHere's the full interaction log with my notations and comments, highlighting notable phenomenons happens in Gemini's answer:",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-11-03T10:24:52.673823+11:00",
    "reply_count": 0,
    "view_count": 109,
    "user": {
      "id": 1762083,
      "name": "Ruizhe Song",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7244375",
    "metrics": {
      "homework_id": "HW2",
      "model_name": "Unknown / Multiple",
      "primary_focus": "model_performance",
      "depth_bucket": "medium",
      "actionability_bucket": "low",
      "word_count": 219
    }
  },
  {
    "id": 7243310,
    "number": 216,
    "course_id": 84647,
    "title": "Special Participation A: HW5 With the Help of Claude AI",
    "document": "Hi, I just made a script documenting how I guided Claude AI to walk through homework 5. I have to acknowledge that Claude AI is a very powerful tool that can help us walk through the homework. I rarely see any hallucinations or misconceptions. Most of the arguments it made are consistent with the right solution especially when the questions are fairly straightforward. However, when it is asked to derive something that requires many intermediate steps, it will sometimes fail to recognize the most obvious thing to do at some point. That is to say, for long-horizon tasks, it tends to make its solutions more complex even though such complex reasoning is not ideal.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-11-03T07:26:53.861759+11:00",
    "reply_count": 0,
    "view_count": 97,
    "user": {
      "id": 970785,
      "name": "Yuxiang Liu",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7243310",
    "metrics": {
      "homework_id": "HW5",
      "model_name": "Claude",
      "primary_focus": "model_performance",
      "depth_bucket": "medium",
      "actionability_bucket": "low",
      "word_count": 114
    }
  },
  {
    "id": 7227387,
    "number": 199,
    "course_id": 84647,
    "title": "Special Participation A: Deepseek Chat on HW3",
    "document": "Conclusion:\n\nThe evaluation of DeepSeek's capabilities for homework 3 has demonstrated :\n\nStrong Mathematical Problem-Solving: DeepSeek reliably handles the mathematical problems, including linear algebra & calculus & probability , providing both solutions and clear, step-by-step explanations. Both the calculation and proof problems were well done. It can retrieve and understand the formulas from papers and analyze them well.\n\nEffective Information Retrieval and Synthesis: A key strength is its ability to process academic papers, accurately identify core arguments and results, and summarize them concisely, demonstrating strong comprehension and distillation skills.\n\nDeepSeek's integration of these capabilities makes it a remarkably efficient for navigating the challenges of advanced math & algorithm & deep learning studies.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-10-30T16:12:13.694906+11:00",
    "reply_count": 0,
    "view_count": 114,
    "user": {
      "id": 1763610,
      "name": "Zhuangzhe Wu",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7227387",
    "metrics": {
      "homework_id": "HW3",
      "model_name": "DeepSeek",
      "primary_focus": "model_performance",
      "depth_bucket": "low",
      "actionability_bucket": "low",
      "word_count": 112
    }
  },
  {
    "id": 7219478,
    "number": 190,
    "course_id": 84647,
    "title": "Special Participation A: ChatGPT on HW1",
    "document": "I used ChatGPT on HW1 no-cording parts(Special Participation A).\nI posted the results, my findings about them, and my summary on the attached pdf.\n\nThis is summary from the pdf.\nOverall, ChatGPT achieved fully correct answers for all problems, demonstrating strong mathematical reasoning and consistency across sequential tasks. However, the interaction also revealed distinctive behavioral patterns in how the model approached problem solving.\n\nAccuracy and One-Shot Performance\n\nFor straightforward conceptual or definitional questions (e.g., explaining optimizer structures or interpreting results), ChatGPT often produced the correct answer immediately (“one-shot”).\n\nFor more complex derivations or matrix manipulations, it sometimes made conceptual simplification errors rather than arithmetic ones. Nevertheless, this resulted in only one error out of seven big problems.\n\nExample of Conceptual Oversimplification\n\nIn one problem, the model incorrectly dropped the matrix when simplifying to .\n\nThis error propagated through subsequent parts, showing that ChatGPT solved sequentially, building upon its previous reasoning.\n\nOnce provided with a hint, it immediately corrected the mistake—illustrating that the model is particularly strong at conditional reasoning with scaffolding.\n\nStyle of Reasoning\n\nChatGPT consistently produced mathematically formal and symbol-heavy responses, sometimes more rigorous than the problem required.\n\nEspecially for “explanation” questions, it tended to over-mathematize rather than summarize intuitively. This pattern was observed across multiple sub-questions.\n\nResponse Latency and Thoughtfulness\n\nThe model typically took 2–5 minutes for moderately complex derivations, suggesting that its “Thinking” mode encouraged stepwise symbolic reasoning rather than rapid generation.\n\nNotation and Presentation\n\nAll responses were logically consistent with the official solutions, though small notational differences (e.g., transposition order or matrix symbols) occasionally appeared.\n\nWhen a matrix identity was mentioned but not proven in the problem statement, ChatGPT sometimes derived it explicitly, reflecting uncertainty about whether it could assume the result.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-10-29T11:02:12.9722+11:00",
    "reply_count": 0,
    "view_count": 99,
    "user": {
      "id": 1308098,
      "name": "Junya Tsuneishi",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7219478",
    "metrics": {
      "homework_id": "HW1",
      "model_name": "ChatGPT (other)",
      "primary_focus": "model_performance",
      "depth_bucket": "medium",
      "actionability_bucket": "medium",
      "word_count": 286
    }
  },
  {
    "id": 7212198,
    "number": 184,
    "course_id": 84647,
    "title": "Special Participation A HW5: ChatGPT 5",
    "document": "Link to the discussion: https://chatgpt.com/share/68ffefde-4c64-800f-9d99-5417df7eb2b8\n\nAcross this project, GPT accuracy was strongest when my prompts were concrete about goals, constraints, and style, and I consistently drove that clarity. I asked questions in a structured way—first “understand → summarize → derive → implement,” then “complete the code” with strict guardrails (no new helpers, inverted dropout, 0.5 L2 factor, BN modes, minimal diffs, NumPy-only where required). GPT answered by laying out the full math (OLS, BN, dropout expected-risk, conv/transpose-conv) and then mirroring it in code that matched the exected methods in different files. When GPT drifted—e.g., offering high-level guidance instead of finished cells/files, or being imprecise about BN mode keys, I nudged the GPT (“complete the notebook/file,” “set bn_param['mode'],” “use inverted dropout and scale by 1/(1-p),” “don’t regularize gamma/beta”), which corrected the approach and aligned outputs the expected result. That feedback loop precise nudges on the few inaccuracies and my immediate revisions in different branches of the same chat , tightened both the math explanations and the implementations until they matched my expected results and coding style.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-10-28T09:29:12.904783+11:00",
    "reply_count": 1,
    "view_count": 172,
    "user": {
      "id": 228039,
      "name": "Mehul Jaiswal",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7212198",
    "metrics": {
      "homework_id": "HW5",
      "model_name": "Unknown / Multiple",
      "primary_focus": "model_performance",
      "depth_bucket": "medium",
      "actionability_bucket": "low",
      "word_count": 175
    }
  },
  {
    "id": 7212131,
    "number": 183,
    "course_id": 84647,
    "title": "Special Participation A: Mistral AI's Le Chat on HW3",
    "document": "Here is the online link: https://chat.mistral.ai/chat/8c72d241-dd44-41a0-b8fc-a0469d84ff1d\n\nHere is the annotated log:\n\nExecutive Summary:\n\nFrom my observation, Le Chat was able to answer most written questions correctly on one shot. However, for questions that reference an external research paper, it could misunderstand the problem statement and draw something tangent to what the question is asking. In particular, it could reference a table on a different page or a formula in a different section. Doing some prompt engineering helps the model to reference the correct table/figure.\n\nIn addition, for questions that involve numerical counting, it could mistake the computation by a small margin, even after engineering the prompt. For instance, it could count something twice and mess up with the calculation.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-10-28T09:17:55.632059+11:00",
    "reply_count": 0,
    "view_count": 146,
    "user": {
      "id": 975971,
      "name": "Jeffrey Cheng",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7212131",
    "metrics": {
      "homework_id": "HW3",
      "model_name": "Unknown / Multiple",
      "primary_focus": "model_performance",
      "depth_bucket": "low",
      "actionability_bucket": "medium",
      "word_count": 119
    }
  },
  {
    "id": 7202422,
    "number": 182,
    "course_id": 84647,
    "title": "Special Participation A: Deepseek on HW5",
    "document": "For the HW5, I try to use Deepseek to solve the problem sets and all the questions and answers are documented in the above files. The strategies I am using are simple, just putting the transcription of the problem to Deepseek and see how it can solve the problems. There are some traits of the Deepseek AI:\n\n1. Deepseek does not support Multi-modal input, therefore when the problems rely on image input, then the model cannot really give a proper answer.\n2. Ambiguous question prompts will lead to longer thinking time.\n3. Deepseek is better at explanation than real calculation. My recommendations for using Deepseek as a learning tool to make it help with conceptual understanding.\n\nThis is observed from some of the facts:\n\n1. Of the 11 questions, Deepseek gets 9 of them correct first shot, while one of the question relies on vision input and the other is a calculation problem. It gets the calculation problem correct after another prompt of asking itself to self examine the calculation process. \n2. I try to ask Deepseek examines its own mistake after I finish all the problems and it still can directly and precisely locate where might go wrong, which demonstrate its capable long context understanding capability.\n3. For problem 4, Deepseek not only provides very clear demonstration on how to derive the answer correctly and can show the direct relation between batchnorm and dropout by directly deriving the scaling factor.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-10-26T16:13:16.614618+11:00",
    "reply_count": 0,
    "view_count": 134,
    "user": {
      "id": 1751442,
      "name": "Jerry Xiao",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7202422",
    "metrics": {
      "homework_id": "HW5",
      "model_name": "DeepSeek",
      "primary_focus": "model_performance",
      "depth_bucket": "medium",
      "actionability_bucket": "medium",
      "word_count": 241
    }
  },
  {
    "id": 7162279,
    "number": 166,
    "course_id": 84647,
    "title": "Special Participation A: Grok on HW0",
    "document": "For the special participation A on HW0, I use Grok to address the non-coding analytical components (problems 2–5). The performance of Grok really impressed me, almost all questions are one-shot correct except for 5(b)(iii).\n\nSummary: Grok demonstrated reliable analytical reasoning for most problems, with clear and accurate derivations for problems 2, 3, and 4. The answer of them are all fully correct. However, the error on problem 5(b)(iii) suggests its sensitivity to complex or ambiguous problem structures. \n\nHere is the link https://grok.com/share/c2hhcmQtMg%3D%3D_935edffc-4c41-4eb5-931a-aaf30ccdf737",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-10-20T18:33:07.202349+11:00",
    "reply_count": 0,
    "view_count": 219,
    "user": {
      "id": 1753326,
      "name": "Tianyu Gu",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7162279",
    "metrics": {
      "homework_id": "HW0",
      "model_name": "Unknown / Multiple",
      "primary_focus": "model_performance",
      "depth_bucket": "medium",
      "actionability_bucket": "medium",
      "word_count": 82
    }
  },
  {
    "id": 7151370,
    "number": 163,
    "course_id": 84647,
    "title": "Special Participation A: GPT-Oss on HW5",
    "document": "For this special participation, I used gpt-oss-120b (Reasoning = High) to solve all non-coding analytical parts of HW5 (Q1–Q4).\nThe model was tested on symbolic derivations and conceptual reasoning without code execution.\n\nAccuracy: 9 / 11 one-shot (82 %), 2 / 11 minor-nudge (18 %)\n\nMain errors: one ASCII matrix mis-parse and one BatchNorm vs LayerNorm confusion\n\nNo hallucinations: all final answers matched the official HW5 solution key\n\nTakeaway: once prompts were clearly formatted in LaTeX, the model produced correct and interpretable derivations, showing reliable analytical reasoning but high sensitivity to input structure.\n\nBit slow(90-180 seconds)\n\n(Logs and annotations are attached in the full PDF report.)",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-10-18T11:31:49.638758+11:00",
    "reply_count": 0,
    "view_count": 240,
    "user": {
      "id": 1770934,
      "name": "Noah Lund Syrdal",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7151370",
    "metrics": {
      "homework_id": "HW5",
      "model_name": "Unknown / Multiple",
      "primary_focus": "model_performance",
      "depth_bucket": "medium",
      "actionability_bucket": "low",
      "word_count": 106
    }
  },
  {
    "id": 7148413,
    "number": 162,
    "course_id": 84647,
    "title": "Special Participation A: Grok on HW5",
    "document": "This document is my report for the HW5 - written part. I evaluate the performance of Grok. It is tested across a series of theoretical deep learning questions covering topics such as convolutional networks, batch normalization, dropout, depthwise separable convolutions, and regularization. The report includes:\n\nAn overview of Grok’s performance, where I categorized each task into one of four levels: One-Shot Correct, Minor Misconception, Larger Error, or Did Not Solve.\n\nA reflection on Grok’s strengths and weaknesses, where I highlight its strong conceptual intuition, clarity, and adaptability, while noting occasional over-explanation and moments where certain steps could have been emphasized more clearly.\n\nMy recommendations for effectively using Grok as a learning tool, emphasizing the importance of clear prompting, iterative clarification, and maintaining focus on conceptual understanding.\n\nFull transcripts of my Q&A exchanges with Grok, including its solutions, reasoning, and my reflections after each subproblem. \n\nOverall, I conclude that Grok performs really well as a conceptual teaching and problem-solving assistant for theoretical deep learning. It has only minor inefficiencies related to verbosity or explicitness.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-10-18T02:48:56.469848+11:00",
    "reply_count": 0,
    "view_count": 198,
    "user": {
      "id": 1765812,
      "name": "Anders Vestrum",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7148413",
    "metrics": {
      "homework_id": "HW5",
      "model_name": "Unknown / Multiple",
      "primary_focus": "model_performance",
      "depth_bucket": "medium",
      "actionability_bucket": "medium",
      "word_count": 173
    }
  },
  {
    "id": 7132324,
    "number": 155,
    "course_id": 84647,
    "title": "Special Participation A: HW 4 using DeepSeek",
    "document": "Problem Context\n\nThere are 5 non-coding questions in this homework. Two questions on optimization (specifically Newton-Schultz iteration and MuP scaling) and three questions on CNNs. I took a special interest in how DeepSeek performs when solving these computationally heavy problems when prompted in different formats: purely text, purely images, a PDF and a hybrid of text and images. I also tested DeepSeek's ability to understand and retain multi-part instructions in between these different modalities. The idea was to evaluate DeepSeek's overall problem-solving performance while also attempting to understand which prompt type yielded the best results. \n\nIn this experiment:\n\nDeepSeek solved four out of five questions correctly in a single attempt.\n\nQuestion 2 was the only one it got wrong, though it could solve it with further prompting.\n\nQuestion 3 required a brief nudge from me to complete the final computation, likely because the context limit was reached mid-session.\n\nExecutive Summary\n\nFrom testing out different formats of the prompts, I observed the following:\n\nWhen only text was provided (Question 2), DeepSeek tended to lose context more easily. it sometimes failed to connect earlier parts of the questions or instructions.\n\nWith images or PDFs, it demonstrated significant better context retention and reasoning continuity. It appeared to process the questions more holistically, possibly due to the visual cues.\n\nA hybrid approach (text +images) also performed better. It was able to mix information from images and text efficiently to provide the correct solution (see Question 4).\n\nIn essence, DeepSeek performs impressively well on conceptual and numerical reasoning when the sufficient structured context is provided. Its performance, however, degraded slightly in purely textual prompts, indicating that context formatting plays a role in achieving accurate results.\n\nNote: This analysis is based on a limited sample of five questions, and is therefore not exhaustive or conclusive.\n\nHere is a link to the conversation I had with DeepSeek: https://chat.deepseek.com/share/jsuxr35m8bhnxyzz7b\n\nHere is the file containing the conversation, with question-wise comments (annotated in the PDF):",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-10-15T10:18:16.802913+11:00",
    "reply_count": 0,
    "view_count": 220,
    "user": {
      "id": 1691230,
      "name": "Srikar Babu Gadipudi",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7132324",
    "metrics": {
      "homework_id": "HW4",
      "model_name": "DeepSeek",
      "primary_focus": "model_performance",
      "depth_bucket": "medium",
      "actionability_bucket": "medium",
      "word_count": 325
    }
  },
  {
    "id": 7111658,
    "number": 141,
    "course_id": 84647,
    "title": "Special Participation A - HW 3 ChatGPT 5",
    "document": "Executive Summary\n\nI used ChatGPT 5 to interactively engage with HW 3's problems and get it to get to the correct answer. While this wasn't the first time I've interacted with ChatGPT 5 for Deep Learning material, I was still impressed with its performance. The majority of questions were one-shotted correctly.\n\n1: Maximal Update Parameterization: \n\nSuccessfully one shotted, reasoning good.\n\n3: Maximal Update Parameterization Research:\n\nWhen interpreting figure 1, ChatGPT 5's explanation of the visual figure was actually more informative and explanatory than the official solutions were. It critically highlighted that \"each layer’s updates remain of the same order regardless of width\"\n\nRather than using the RMS-to-RMS matrix norm that we covered in lecture (scaled version of spectral norm), it hallucinated the definition of the RMS matrix norm. It defined it as if the matrix was unrolled into a vector and then the RMS norm of that flattened vector was taken, \n\n$$\\frac{\\left\\lVert W\\right\\rVert_F}{\\sqrt{n_{l-1}n_l}}$$\n\nAs a result, it used this relation (that it claimed applies to random matrices)\n\n\n$$∥W_l∥_2\\approx\\frac{∥W_l∥_F}{\\sqrt{\\left\\{\\min(n_l,n_{l-1})\\right\\}}}$$\n\nto derive the incorrect order for the weight matrix and update's RMS norms.\n\nWhile it made this error, ChatGPT 5 was still able to correctly derive the upper bounds in Desideratum 1 (because it didn't use the RMS norms of hidden layer vector/update or weight matrix/update for its derivation). This is not that surprising given that I fed in the \"A Spectral Condition for Feature Learning\" pdf at the beginning of the chat and ChatGPT produced a very similar derivation. Additionally, it was still able to correctly identified the key assumption that allowed for the derivation of the lower bound.\n\nAdditionally, after providing the definition of the induced norm, it was able to correctly derive the  order for the weight matrix and update's RMS norms.\n\n4: Policy Gradient and the Reparameterization Gradient Estimator:\n\n Overall, ChatGPT5 seemed to provide comprehensive reasoning (and more additional information/context) and got to the correct solutions. \n\n5: Tensor Rematerialization:\nChatGPT 5 at first guessed 2 loadmems instead of 10 (thinking once a layer was loaded, it would stay in memory for the next layer). After I reiterated the problem statement, it was easily able to answer and explain the total number of loadmems as 10.\n\nOther than that, it was able to provide sound reasoning and the correct answers for the rest of the problem\n\nOverall:\nChatGPT seemed to provide comprehensive reasoning for every problem (sometimes an unnecessary amount), and seemed to be able to course-correct with a little bit of feedback. It seemed to falter more on topics or ideas that had less public material available online (e.g. RMS-RMS matrix norm)",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-10-11T14:07:05.580704+11:00",
    "reply_count": 0,
    "view_count": 191,
    "user": {
      "id": 1288128,
      "name": "Iana Lin",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7111658",
    "metrics": {
      "homework_id": "HW3",
      "model_name": "Unknown / Multiple",
      "primary_focus": "model_performance",
      "depth_bucket": "medium",
      "actionability_bucket": "medium",
      "word_count": 434
    }
  },
  {
    "id": 7095749,
    "number": 134,
    "course_id": 84647,
    "title": "Special Participation A: - Deepseek on HW1",
    "document": "Intro:\nI'm using Deepseek to solve HW1, including 7 problems.\n\nMy prompts:\n\n1. Please help me deal with these problems about deep neural networks. Think it step by step.\n\n<answer> (Given the solution of Problem 1)\n\n2. You have done a good job! What about the remaining problems? Think it step by step.\n\n<answer> (Given the solution of Problem 2)\n\n3.  Yes, proceed with Problem 3 about momentum.\n\n<answer>(Given the solution of Problem 3)\n\n4. Plz, continue.\n<answer>(Given the solution of Problem 4-7)\n\nMy observation:\n\n1. When the thinking and answer content was too long, DeepSeek only answered a part of the problems, probably due to the chunking technique and LLM's inclination to output shorter contents. So that's why there're 4 prompts for HW1.\n\n2. The accuracy is very high, especially for the computational tasks. However, If format of the question is not ordinary, it will be ignored sometimes, such as 4(a).\n\nI don't know whether it can be called \"hallucination\", but actually in the thinking content, it indeed notices this task but considers 4(a) internally without giving the solution: \n\n3. A serious problem occurs that at times, Deepseek will unnoticeably make an assumption, even if there is no information about it. This problem fails the COT(Chain of Thought) if the assumption is not expected, leading to the wrong answer. More seriously, if the users don't look through the process of thinking, they may heartily accept the answer, but the conflict is that the thinking content is too long to read.\n\n4. Deepseek fails to perfectly answer ambiguous questions, such as 2(c).  It seems that it tends to give a conceptual, verbal description, if possible. It can be seen as a kind of laziness from my perspective.\n\nLet's first compare the standard answer and Deepseek's answer:\n\nStandard： \n\nDeepseek:\n\nAlthough the two solutions points to the same conclusion, Deepseek's answer is quite short. I guessed that Deepseek mixed up the identity of reader. Therefore, I asked it to rewrite the solution with different identity prompting:\n\n--------------\n\nPrompt1:\n\nYou are a student doing the assignment. Please rewrite Question 2(c).\n\nPrompt2:\n\nThe professor will read your solution! Please rewrite Question 2(c).\n\n--------------\n\nBut the result shows that Deepseek still gives conceptual answers, only adding more description. Only when I mentioned 'calculation' externally, It does work. \n\n\n\nRecommendation:\n\nFor Observation 1: \n\nIf there's no question number, the chunking will be unpredictable. Therefore , I recommend managing long problems with explicit chunking by yourself.\n\nFor Observation 2:\n\nI don't know how to solve this problem. I think reducing the use of table may help.\n\nFor Observation 3:\n\nRequire DeepSeek to state assumptions explicitly. But sometimes, the reasoning chain will be too long so that perhaps looking through it by yourself is also another good choice. \n\nFor Observation 4:\n\nHandle ambiguity with targeted instruction.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-10-09T03:33:14.844297+11:00",
    "reply_count": 2,
    "view_count": 214,
    "user": {
      "id": 1751485,
      "name": "Tianhao Qian",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7095749",
    "metrics": {
      "homework_id": "HW1",
      "model_name": "Unknown / Multiple",
      "primary_focus": "model_performance",
      "depth_bucket": "high",
      "actionability_bucket": "medium",
      "word_count": 467
    }
  },
  {
    "id": 7083805,
    "number": 124,
    "course_id": 84647,
    "title": "Special Participation A: Qwen3-Max on HW0",
    "document": "Here is the online link: https://chat.qwen.ai/s/6240e96b-585c-4943-870d-3af47859ec5f?fev=0.0.222 Here is annotated log: https://drive.google.com/file/d/1vVCY_yCtDPNHagaoSDGR6BSnjG78wVpI/view?usp=sharing Executive Summary: Qwen3-Max is generally able to one-shot nearly all answers and does not have many misconceptions/hallucinations. The main issue is that it often skips steps, which is especially problematic for problems where you ask the model to show that A = B rather than just solve a problem, since the key is in the detailed steps. This was especially the case for “4. The 5 Interpretations of Ridge Regression” where you want to show how the concepts tie together. Prompting it repeatedly to show more detailed steps helped.\n\nIt’s also important to be careful about pasting in from the PDF as the copy paste can lead the model to parse text incorrectly, where I needed to correct the model to consider “||w||^2 not ||w||_2”.\n\nFinally, it struggled greatly with “5. ReLU Elbow Update under SGD”. In particular, it attempted to draw and label using ascii art, but the plots were messed up and even after repeated prompting, they are still not presentable.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-10-07T08:03:27.722243+11:00",
    "reply_count": 0,
    "view_count": 143,
    "user": {
      "id": 1683657,
      "name": "Andy Zhang",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7083805",
    "metrics": {
      "homework_id": "HW0",
      "model_name": "Unknown / Multiple",
      "primary_focus": "model_performance",
      "depth_bucket": "medium",
      "actionability_bucket": "medium",
      "word_count": 174
    }
  },
  {
    "id": 7074543,
    "number": 116,
    "course_id": 84647,
    "title": "Special Participation A: Deepseek with Deep Thinking on HW0",
    "document": "Here is the online link: https://chat.deepseek.com/share/hcxrv1b7tn9s8c3lo0\n\nHere is my annotated version of the log: https://drive.google.com/file/d/18ZU3GgmdtP_u84GnVxof8Kh2yrbpOotN/view?usp=sharing\n\nExecutive Summary:\nFrom my observations, at least for DeepSeek, the model tends to capture most of the details of the problem-solving process within its internal reasoning. However, even after some targeted prompt engineering, it often fails to provide a detailed explanation of its reasoning and solution in the actual response to the user. At times, it even skips crucial steps in deriving mathematical expressions. For example, when asked for the minimum norm solution of ridge regression, it outputs the correct expression but provides little explanation of how it was derived.\n\nMost of the time, the model answers in a single step rather than engaging in a back-and-forth process to verify whether its solution is fully correct. This one-shot approach can be acceptable for straightforward problems, but it does cause errors in more complex cases. This issue is visible later in the annotated log, where the model fails to consider multiple possible cases for a single question.\n\nOn the positive side, I did not notice any hallucinations so far.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-10-05T19:56:43.327041+11:00",
    "reply_count": 1,
    "view_count": 208,
    "user": {
      "id": 647731,
      "name": "Wesley Kai Zheng",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7074543",
    "metrics": {
      "homework_id": "HW0",
      "model_name": "DeepSeek",
      "primary_focus": "model_performance",
      "depth_bucket": "medium",
      "actionability_bucket": "medium",
      "word_count": 183
    }
  },
  {
    "id": 7049136,
    "number": 106,
    "course_id": 84647,
    "title": "Special Participation A - Grok HW3",
    "document": "REFLECTION\n\n​​In completing the non-coding parts of the homework with Grok, I found that it could one-shot questions about 70–80% of the time. Hints proved extremely useful, as providing a small nudge almost always led to a correct answer, while leaving them out sometimes still allowed Grok to succeed, such as on questions 4C and 4D. This showed that while hints can significantly improve accuracy, the model is capable of reasoning independently in some cases. I also noticed that the model tended to give very long and verbose responses, often providing explanations two to three pages long, even when a short answer would have sufficed. This happened especially when I asked follow-up questions, and Grok would completely lose focus.\n\nOne particularly striking behavior was how Grok interacted over the course of the chat. When I gave feedback—acknowledging correct answers or asking for more details—it seemed to “learn” from the previous interaction, adjusting its responses accordingly. However, this sometimes caused it to overextend or go beyond the original scope of a question. For instance, in question five, Grok spent over seven minutes thinking about an answer before I had even formally asked the question, showing both its ambition to solve problems and its tendency to act preemptively. Similarly, when I asked multiple follow-up questions, it occasionally lost focus on the original prompt, producing responses that were less relevant or harder to parse.\n\nOverall, my experience with Grok highlighted both its strengths and its quirks. It consistently produced correct answers for most questions, and strategic prompting and providing feedback helped guide it effectively. At the same time, its verbosity, occasional hallucinations, and overzealous problem-solving emphasized the need for careful interaction and moderation. Despite these limitations, Grok was a highly capable tool for this homework, and the process offered valuable insights into how modern LLMs balance reasoning, responsiveness, and conciseness in an interactive problem-solving setting.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-10-01T09:44:26.995833+10:00",
    "reply_count": 1,
    "view_count": 160,
    "user": {
      "id": 647432,
      "name": "Bruno Vieira",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7049136",
    "metrics": {
      "homework_id": "HW3",
      "model_name": "LLM (unspecified)",
      "primary_focus": "model_performance",
      "depth_bucket": "medium",
      "actionability_bucket": "medium",
      "word_count": 312
    }
  }
]